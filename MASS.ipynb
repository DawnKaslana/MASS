{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 基于MindSpore框架的MASS案例实现\n",
    "## 1 模型简介\n",
    "微软亚洲研究院于2019在ICML发表《MASS: Masked Sequence to Sequence Pre-training for Language Generation》，其借鑑了Bert的Masked Language Model预训练任务，提出了MAsked Sequence to Sequence Pre-training（MASS）模型，为自然语言生成任务联合预训练编码器和解码器。\n",
    "\n",
    "MASS的编码器-解码器结构示例，图中“_”表示被屏蔽的词。\n",
    "![](https://i.imgur.com/Jvhm0Dx.png)\n",
    "编码器： 以被随机屏蔽掉连续片段的句子作为输入，BERT的做法是随机屏蔽掉15%的词，而MASS为了解决编码与解码之间的平衡，做法为屏蔽掉句子总长50%的片段。模型中使用特殊符号$[\\mathbb M]$替换连续的单词来屏蔽片段，起始位置是随机的，且被选中的token有80%的概率是正常的$[\\mathbb M]$token，10%的概率是被随机token替换，10%的概率保持原来的token。以上图为例，其中输入序列有8个单词，片段$x_3-x_6$被屏蔽掉。\n",
    "\n",
    "解码器：输入为与编码器同样的序列，但是会屏蔽掉剩馀的词，然后解码器只预测编码器端屏蔽掉的词。以上图为例，只给定 $x_3x_4x_5$ 作为位置 4 - 6 的解码器输入，解码器会将 $[\\mathbb M]$ 作为其他位置的输入（屏蔽了位置 1 − 3 和 7 − 8）。为了减少内存和计算成本，被屏蔽的token会被移除，未屏蔽token的位置编码不变（如果前两个标记被屏蔽并移除，第三个标记的位置仍然是 2 而不是 0)。通过这种方式，可以获得相似的准确度，并在解码器中减少 50% 的计算量。\n",
    "\n",
    "\n",
    "```\n",
    "encoder input (source): [x1, x2, x3, x4, x5, x6, x7, x8, </eos>]\n",
    "masked encoder input:   [x1, x2, x3,  _,  _,  _, x7, x8, </eos>]\n",
    "decoder input:          [  -, x3, x4, x5]\n",
    "                          |   |   |   |\n",
    "                          V   V   V   V\n",
    "decoder output:         [x3, x4, x5, x6]\n",
    "```\n",
    "\n",
    "MASS预训练有以下几大优势：\n",
    "\n",
    "(1) 编码器被强制去抽取未被屏蔽掉的词的含义，可以提升编码器理解源序列文本的能力。\n",
    "\n",
    "(2) 通过在解码器端预测连续的标记，解码器可以比仅预测离散标记拥有更好的语言建模能力。\n",
    "\n",
    "(3) 通过在解码器端进一步屏蔽在编码器端未被屏蔽掉的词， 以鼓励解码器从编码器端提取更多有用的信息来做预测，而不是依赖于前面预测出的单词，这样能促进编码器-解码器结构的联合训练。\n",
    "\n",
    "### 1.1 模型结构\n",
    "\n",
    "其模型基础结构可以使用任何Seq2Seq的结构，由于Transformer的优越性，故论文中使用Transformer模型作为基础结构，Transformer整体架构由 Encoder 和 Decoder 两个部分组成，不依赖任何RNN和CNN结构来生成输出，而是使用了Attention注意力机制，自动学习输入序列中每个单词和其他单词的关联，可以更好的处理长文本，且该模型可以高效的并行工作，训练速度较快。\n",
    "\n",
    "Transformer 的整体架构如下：\n",
    "\n",
    "<center>\n",
    "<img src='https://i.imgur.com/ooO7ULP.png' height='600px'>\n",
    "</center>\n",
    "\n",
    "- 编码器和解码器分别由$N=6$个相同的编码器/解码器层组成。\n",
    "- 在 Transformer 架构的左半部分，编码器的任务是将输入序列映射到一系列连续表示，然后将其馈送到解码器。\n",
    "- 架构右半部分的解码器接收编码器的输出以及前一个时间步的解码器输出，以生成输出序列。\n",
    "- 解码器的输出最终通过一个全连接层，然后是一个 softmax 层，以生成对输出序列下一个单词的预测。\n",
    "\n",
    "### 1.2目标函数\n",
    "\n",
    "给定一个未配对的源句子$x ∈ \\mathcal X$，通过被屏蔽的序列$x^{\\setminus u:v}$作为输入来预测句子片段$x^{u:v}$以预训练序列到序列模型。以极大似然函数作为目标函数：\n",
    "\n",
    "$$\n",
    "L(\\theta; \\mathcal X) = \\frac{1}{ |\\mathcal X|} \\sum_{x\\in \\mathcal X} \\log P(x^{u:v}|x^{\\setminus u:v};\\theta)\\\\\n",
    "= \\frac{1}{|\\mathcal X|}\\log \\Pi_{t=u}^{v} P(x^{u:v}_t|x^{u:v}_{<t}, x^{\\setminus u:v};\\theta).\n",
    "$$\n",
    "\n",
    "\n",
    "注: $x^{u:v}$表示以句子位置$u$为起点$v$为终点的片段；$x^{\\setminus u:v}$为$x^{u:v}$的修改版本，从$u$到$v$的片段被屏蔽，$0 < u < v < m$ 其中 $m$ 是句子 $x$ 长度。\n",
    "\n",
    "### 1.3 模型特点\n",
    "MASS 有一个重要的超参数 $k$，表示屏蔽的连续片段长度，通过调整 $k$ 的大小，MASS 能包含 BERT 中的掩码语言模型训练方法以及 GPT 中标准的语言模型预训练方法，使 MASS 成为一个通用的预训练框架。\n",
    "\n",
    "当 $k = 1$ 时，根据MASS的设定，编码器端仅屏蔽一个单词，解码器以源序列中未屏蔽的单词为条件预测这个单词，如图(a)所示。由于解码器的所有输入都被屏蔽了，因此解码器本身就像一个非线性分类器，类似于 BERT 中使用的 softmax 矩阵。在这种情况下，条件概率为 $P (x^u|x^{\\setminus u}; θ)$，$u$是掩码标记的位置，这正是 BERT3中使用的掩码语言模型的公式。\n",
    "\n",
    "![](https://i.imgur.com/0QNrDSJ.png)\n",
    "\n",
    "当 $k = m$（ $m$ 为序列长度）时，根据MASS的设定，编码器会屏蔽所有的单词，解码器需要预测所有单词，如图(b)所示。由于编码器端所有词都被屏蔽了，解码器的注意力机制相当于没有获取到信息，在这种情况下条件概率为 $P(x^{1:m}|x^{\\setminus 1:m}; θ)$，等价于GPT中的标准语言模型。\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/2Q4pbLy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2案例实现\n",
    "\n",
    "### 2.1 環境建置\n",
    "pip install -r requirements.txt\n",
    "\n",
    "### 2.2 准备数据集\n",
    "案例实现中预训练模型所使用的数据即News Crawl的英语单语数据数据集，下载好的数据集为一纯文字文件，接下来需要对该数据进行预处理，预处理包括对数据进行分词、利用subword-nmt工具做bpe编码、对分词后的语料应用该bpe编码并构建词彙表等工作。\n",
    "\n",
    "而微调模型用于文本摘要任务所使用的数据集为Gigaword，该数据集已经有分割为训练、测试、验证集，有原文本(src)和目标摘要(tgt)两个文件，本案例只会使用训练及与测试集，数据集文件路径结构如下：\n",
    "\n",
    ".Dataset/<br>\n",
    "└── news_crawl<br>\n",
    "&emsp;&emsp;└── news.2015.txt<br>\n",
    "└── ggw_data<br>\n",
    "&emsp;&emsp;├── test.src.txt<br>\n",
    "&emsp;&emsp;├── test.tgt.txt<br>\n",
    "&emsp;&emsp;├── train.src.txt<br>\n",
    "&emsp;&emsp;└── train.tgt.txt<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/dawnkaslana/Workspace/Dataset/news_crawl/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3861/902327193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" | Wrote to {tokenized_file}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/dawnkaslana/Workspace/Dataset/news_crawl/'"
     ]
    }
   ],
   "source": [
    "\"\"\"對數據進行分詞\"\"\"\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "src_folder = \"/Users/dawnkaslana/Workspace/Dataset/news_crawl/\"\n",
    "out_folder = \"./tokenized_corpus/\"\n",
    "\n",
    "def create_tokenized_sentences(file_path, tokenized_file):\n",
    "    tokenized_sen = []\n",
    "    print(f\" | Processing {file_path}.\")\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for sen in file:\n",
    "            tokens = word_tokenize(sen)\n",
    "            tokens = [t for t in tokens if t != \" \"]\n",
    "            if len(tokens) > 175:\n",
    "                continue\n",
    "            tokenized_sen.append(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "    with open(tokenized_file, \"w\") as file:\n",
    "        file.writelines(tokenized_sen)\n",
    "    print(f\" | Wrote to {tokenized_file}.\")\n",
    "\n",
    "for file in os.listdir(src_folder):\n",
    "    if not file.endswith(\".txt\"):\n",
    "        continue\n",
    "    file_path = os.path.join(src_folder, file)\n",
    "    tokenized_file = os.path.join(out_folder, file.replace(\".txt\", \"_tokenized.txt\"))\n",
    "    create_tokenized_sentences(file_path, tokenized_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"利用subword-nmt工具生成bpe檔案\"\"\"\n",
    "src_folder_path = '/Users/dawnkaslana/Workspace/Dataset/news_crawl/' # source text folder path.\n",
    "os.system(\"cd %s && cat *.txt | subword-nmt learn-bpe -s 46000 -o all.bpe.codes\" % (src_folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"應用該bpe檔案並構建詞彙表.\"\"\"\n",
    "from src.utils import Dictionary\n",
    "import subprocess\n",
    "\n",
    "source_folder = os.path.abspath(\"./tokenized_corpus/\")\n",
    "output_folder = os.path.abspath(\"./tokenized_corpus/bpe/\")\n",
    "codes = os.path.abspath(\"./all.bpe.codes\")\n",
    "vocab_path = \"./vocab/vocab_en.dict.bin\"\n",
    "\n",
    "ENCODER = \"subword-nmt apply-bpe -c\"\n",
    "LEARN_DICT = \"subword-nmt get-vocab -i\"\n",
    "def bpe_encode(codes_path, src_path, output_path, dict_path):\n",
    "    # Encoding.\n",
    "    print(\" | Applying BPE encoding.\")\n",
    "    commands = ENCODER.split() + [codes_path] + [\"-i\"] + [src_path] + [\"-o\"] + [output_path]\n",
    "    subprocess.call(commands)\n",
    "    print(\" | Fetching vocabulary from single file.\")\n",
    "    # Learn vocab.\n",
    "    commands = LEARN_DICT.split() + [output_path] + [\"-o\"] + [dict_path]\n",
    "    subprocess.call(commands)\n",
    "\n",
    "available_dict = []\n",
    "for file in os.listdir(source_folder):\n",
    "    if file.endswith(\".txt\"):\n",
    "        output_path = os.path.join(output_folder, file.replace(\".txt\", \"_bpe.txt\"))\n",
    "        dict_path = os.path.join(output_folder, file.replace(\".txt\", \".dict\"))\n",
    "        available_dict.append(dict_path)\n",
    "        bpe_encode(codes, os.path.join(source_folder, file), output_path, dict_path)\n",
    "\n",
    "# 加载bpe_encode處理過的文本词汇表，行格式为word frequency。\n",
    "vocab = Dictionary.load_from_text(available_dict)\n",
    "vocab.persistence(vocab_path) #将词汇表对象保存为二进制文件。\n",
    "print(f\" | Vocabulary Size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 生成NewsCrawl數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Processing corpus /Users/dawnkaslana/Workspace/Dataset/news_crawl/news2007.txt.\n",
      " | Shortest len = 1.\n",
      " | Longest  len = 3269.\n",
      " | Total    sen = 2573547.\n",
      " | Write to /Users/dawnkaslana/Workspace/HWmodels/official/nlp/mass/train_data/news_crawl_dataset/news2007_len_32.tfrecord-001-of-001.\n",
      " | Generate Dataset for Pre-training is done.\n",
      " | Vocabulary size: 353.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create News Crawl Pre-Training Dataset.\"\"\"\n",
    "import os\n",
    "from src.dataset import MonoLingualDataLoader\n",
    "from src.language_model import LooseMaskedLanguageModel\n",
    "from src.utils import Dictionary\n",
    "\n",
    "input_folder_path = '/Users/dawnkaslana/Workspace/Dataset/news_crawl/'\n",
    "output_folder_path = './train_data/news_crawl_dataset/'\n",
    "vocab_path = './vocab/vocab_en.dict.bin'\n",
    "\n",
    "def create_pre_train(text_file, max_sen_len):\n",
    "    vocab = Dictionary.load_from_persisted_dict(vocab_path)\n",
    "\n",
    "    loader = MonoLingualDataLoader(\n",
    "        src_filepath=text_file,\n",
    "        lang=\"en\", dictionary=vocab,\n",
    "        language_model=LooseMaskedLanguageModel(mask_ratio=0.4, mask_all_prob=None),\n",
    "        max_sen_len=max_sen_len, min_sen_len=10\n",
    "    )\n",
    "\n",
    "    src_file_name = os.path.basename(text_file)\n",
    "\n",
    "    file_name = os.path.join(\n",
    "        output_folder_path,\n",
    "        src_file_name.replace('.txt', f'_len_{max_sen_len}.tfrecord')\n",
    "    )\n",
    "    loader.write_to_tfrecord(path=file_name)\n",
    "\n",
    "for file in os.listdir(input_folder_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        create_pre_train(os.path.join(input_folder_path, file), 32)\n",
    "\n",
    "print(f\" | Generate Dataset for Pre-training is done.\")\n",
    "print(f\" | Vocabulary size: {vocab.size}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 生成Gigaword數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Processing corpus /Users/dawnkaslana/Workspace/Dataset/ggw_data/org_data/train.src.txt.\n",
      " | Processing corpus /Users/dawnkaslana/Workspace/Dataset/ggw_data/org_data/train.tgt.txt.\n",
      " | Shortest len = 3.\n",
      " | Longest  len = 100.\n",
      " | Total    sen = 1981314.\n",
      " | Total token num=87383811, 87.10378401784284% replaced by <unk>.\n",
      " | Write to /Users/dawnkaslana/Workspace/HWmodels/official/nlp/mass/train_data/gigaword_dataset/gigaword_train_dataset.tfrecord-001-of-001.\n",
      " | Processing corpus /Users/dawnkaslana/Workspace/Dataset/ggw_data/org_data/test.src.txt.\n",
      " | Processing corpus /Users/dawnkaslana/Workspace/Dataset/ggw_data/org_data/test.tgt.txt.\n",
      " | Shortest len = 2.\n",
      " | Longest  len = 73.\n",
      " | Total    sen = 1081.\n",
      " | Total token num=46933, 85.92248524492362% replaced by <unk>.\n",
      " | Write to /Users/dawnkaslana/Workspace/HWmodels/official/nlp/mass/train_data/gigaword_dataset/gigaword_test_dataset.tfrecord-001-of-001.\n",
      " | Generate Dataset for fine-tuneing is done.\n",
      " | Vocabulary size: 353.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate Gigaword dataset.\"\"\"\n",
    "import os\n",
    "from src.dataset import BiLingualDataLoader\n",
    "from src.language_model import NoiseChannelLanguageModel\n",
    "from src.utils import Dictionary\n",
    "\n",
    "input_folder_path = '/Users/dawnkaslana/Workspace/Dataset/ggw_data/'\n",
    "output_folder_path = './train_data/gigaword_dataset/'\n",
    "vocab_path = './vocab/vocab_en.dict.bin'\n",
    "\n",
    "vocab = Dictionary.load_from_persisted_dict(vocab_path)\n",
    "\n",
    "train = BiLingualDataLoader(\n",
    "    src_filepath=os.path.join(input_folder_path,\"train.src.txt\"),\n",
    "    tgt_filepath=os.path.join(input_folder_path,\"train.tgt.txt\"),\n",
    "    src_dict=vocab, tgt_dict=vocab,\n",
    "    src_lang=\"en\", tgt_lang=\"en\",\n",
    "    language_model=NoiseChannelLanguageModel(add_noise_prob=0.),\n",
    "    max_sen_len=32\n",
    ")\n",
    "\n",
    "train.write_to_tfrecord(\n",
    "    path=os.path.join(output_folder_path, \"gigaword_train_dataset.tfrecord\")\n",
    ")\n",
    "\n",
    "test = BiLingualDataLoader(\n",
    "    src_filepath=os.path.join(input_folder_path,\"test.src.txt\"),\n",
    "    tgt_filepath=os.path.join(input_folder_path,\"test.tgt.txt\"),\n",
    "    src_dict=vocab, tgt_dict=vocab,\n",
    "    src_lang=\"en\", tgt_lang=\"en\",\n",
    "    language_model=NoiseChannelLanguageModel(add_noise_prob=0),\n",
    "    max_sen_len=32\n",
    ")\n",
    "\n",
    "test.write_to_tfrecord(\n",
    "    path=os.path.join(output_folder_path, \"gigaword_test_dataset.tfrecord\")\n",
    ")\n",
    "\n",
    "print(f\" | Generate Dataset for fine-tuneing is done.\")\n",
    "print(f\" | Vocabulary size: {vocab.size}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 預訓練\n",
    "\n",
    "bash run_ascend.sh -t t -n 1 -i 1 <br>\n",
    "python train.py --device_target Ascend --output_path './output'\n",
    "\n",
    "\n",
    "## 微调\n",
    "\n",
    "bash run_ascend.sh -t t -n 1 -i 1\n",
    "\n",
    "## 推理\n",
    "\n",
    "bash run_ascend.sh -t i -n 1 -i 1 -o {outputfile}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindSpore version:  1.7.0\n",
      "The result of multiplication calculation is correct, MindSpore has been installed successfully!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
