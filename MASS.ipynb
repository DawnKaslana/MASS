{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 基于MindSpore框架的MASS案例实现\n",
    "## 1 模型简介\n",
    "微软亚洲研究院于2019在ICML发表《MASS: Masked Sequence to Sequence Pre-training for Language Generation》，其借鑑了Bert的Masked Language Model预训练任务，提出了MAsked Sequence to Sequence Pre-training（MASS）模型，为自然语言生成任务联合预训练编码器和解码器。\n",
    "\n",
    "MASS的编码器-解码器结构示例，图中“_”表示被屏蔽的词。\n",
    "\n",
    "<div align=center>\n",
    "<img src='https://i.imgur.com/Jvhm0Dx.png' width='600px'>\n",
    "</div>\n",
    "\n",
    "编码器： 以被随机屏蔽掉连续片段的句子作为输入，BERT的做法是随机屏蔽掉15%的词，而MASS为了解决编码与解码之间的平衡，做法为屏蔽掉句子总长50%的片段。模型中使用特殊符号 $[\\mathbb M]$ 替换连续的单词来屏蔽片段，起始位置是随机的，且被选中的token有80%的概率是正常的 $[\\mathbb M]$ token，10%的概率是被随机token替换，10%的概率保持原来的token。以上图为例，其中输入序列有8个单词，片段 $x_3-x_6$ 被屏蔽掉。\n",
    "\n",
    "解码器：输入为与编码器同样的序列，但是会屏蔽掉剩馀的词，然后解码器只预测编码器端屏蔽掉的词。以上图为例，只给定 $x_3x_4x_5$ 作为位置 4 - 6 的解码器输入，解码器会将  $[\\mathbb M]$ 作为其他位置的输入（屏蔽了位置 1 − 3 和 7 − 8）。为了减少内存和计算成本，被屏蔽的token会被移除，未屏蔽token的位置编码不变（如果前两个标记被屏蔽并移除，第三个标记的位置仍然是 2 而不是 0)。通过这种方式，可以获得相似的准确度，并在解码器中减少 50% 的计算量。\n",
    "\n",
    "```\n",
    "encoder input (source): [x1, x2, x3, x4, x5, x6, x7, x8, </eos>]\n",
    "masked encoder input:   [x1, x2, x3,  _,  _,  _, x7, x8, </eos>]\n",
    "decoder input:          [  -, x3, x4, x5]\n",
    "                          |   |   |   |\n",
    "                          V   V   V   V\n",
    "decoder output:         [x3, x4, x5, x6]\n",
    "```\n",
    "\n",
    "MASS预训练有以下几大优势：\n",
    "\n",
    "(1) 编码器被强制去抽取未被屏蔽掉的词的含义，可以提升编码器理解源序列文本的能力。<br>\n",
    "(2) 通过在解码器端预测连续的标记，解码器可以比仅预测离散标记拥有更好的语言建模能力。<br>\n",
    "(3) 通过在解码器端进一步屏蔽在编码器端未被屏蔽掉的词， 以鼓励解码器从编码器端提取更多有用的信息来做预测，而不是依赖于前面预测出的单词，这样能促进编码器-解码器结构的联合训练。\n",
    "\n",
    "### 1.1 模型结构\n",
    "\n",
    "其模型基础结构可以使用任何Seq2Seq的结构，由于Transformer的优越性，故论文中使用Transformer模型作为基础结构，Transformer整体架构由编码器和解码器两个部分组成，不依赖任何RNN和CNN结构来生成输出，而是使用了Attention注意力机制，自动学习输入序列中每个单词和其他单词的关联，可以更好的处理长文本，且该模型可以高效的并行工作，训练速度较快。\n",
    "\n",
    "Transformer 的整体架构如下：\n",
    "\n",
    "<div align=center>\n",
    "<img src='https://i.imgur.com/ooO7ULP.png' width='400px'>\n",
    "</div>\n",
    "\n",
    "- 编码器和解码器分别由 $N=6$ 个相同的编码器/解码器层组成。\n",
    "- 在 Transformer 架构的左半部分，编码器的任务是将输入序列映射到一系列连续表示，然后将其馈送到解码器。\n",
    "- 架构右半部分的解码器接收编码器的输出以及前一个时间步的解码器输出，以生成输出序列。\n",
    "- 解码器的输出最终通过一个全连接层，然后是一个 softmax 层，以生成对输出序列下一个单词的预测。\n",
    "\n",
    "Transformer的运作过程举例如下图，这是一个由法文翻译为英文的 Transformer，从编码器端输入一句法文“je suis etudiant”，从解码器端则输出翻译出的英文“I am a student”。\n",
    "\n",
    "<div align=center>\n",
    "<img src='https://i.imgur.com/GQKvNAd.png' width='350'/>\n",
    "</div>\n",
    "\n",
    "1. 整句法文“je suis etudiant”做为编码器的输入，编码器一次处理整个句子，产生输出 $X$。\n",
    "2. 解码器以 $X$ 及空序列做为其输入，产生第一个输出 I。\n",
    "3. 解码器以 $X$ 及 I 做为其输入，产生第二个输出 am。\n",
    "4. 解码器以 $X$ 及 I am 做为其输入，产生第三个输出 a。\n",
    "5. 解码器以 $X$ 及 I am a 做为其输入，产生第四个输出 student。\n",
    "6. 解码器以 $X$ 及 I am a student 做为其输入，产生第五个输出 <end of sentence>。\n",
    "\n",
    "\n",
    "### 1.2目标函数\n",
    "\n",
    "给定一个未配对的源句子 $x ∈ \\mathcal X$ ，MASS通过被屏蔽的序列 $x^{\\setminus u:v}$ 作为输入来预测句子片段 $x^{u:v}$ 以预训练序列到序列模型。目标函数为一极大似然函数：\n",
    "\n",
    "$$\n",
    "L(\\theta; \\mathcal X) = \\frac{1}{|\\mathcal X|} \\sum_{x \\in \\mathcal X} \\log P(x^{u:v} | x^{\\setminus u:v}; \\theta) \\\\ = \\frac{1}{|\\mathcal X|} \\log \\Pi^{v}_{t=u} P(x^{u:v}_{t}|x^{u:v}_{\\textless t}, x^{\\setminus u:v};\\theta)\n",
    "$$\n",
    "\n",
    "注: $x^{u:v}$ 表示以句子位置 $u$ 为起点 $v$ 为终点的片段； $x^{\\setminus u:v}$ 为 $x^{u:v}$ 的修改版本，从 $u$ 到 $v$ 的片段被屏蔽， $0 < u < v < m$ 其中 $m$ 是句子 $x$ 长度。\n",
    "\n",
    "### 1.3 模型特点\n",
    "MASS 有一个重要的超参数 $k$，表示屏蔽的连续片段长度，通过调整 $k$ 的大小，MASS 能包含 BERT 中的掩码语言模型训练方法以及 GPT 中标准的语言模型预训练方法，使 MASS 成为一个通用的预训练框架。\n",
    "\n",
    "当 $k = 1$ 时，根据MASS的设定，编码器端仅屏蔽一个单词，解码器以源序列中未屏蔽的单词为条件预测这个单词，如图(a)所示。由于解码器的所有输入都被屏蔽了，因此解码器本身就像一个非线性分类器，类似于 BERT 中使用的 softmax 矩阵。在这种情况下，条件概率为 $P (x^u|x^{\\setminus u}; θ)$， $u$ 是掩码标记的位置，这正是 BERT3中使用的掩码语言模型的公式。\n",
    "\n",
    "当 $k = m$（ $m$ 为序列长度）时，根据MASS的设定，编码器会屏蔽所有的单词，解码器需要预测所有单词，如图(b)所示。由于编码器端所有词都被屏蔽了，解码器的注意力机制相当于没有获取到信息，在这种情况下条件概率为 $P(x^{1:m}|x^{\\setminus 1:m}; θ)$，等价于GPT中的标准语言模型。\n",
    "\n",
    "![](https://i.imgur.com/JdTGb9z.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 模型构建\n",
    "以下我们使用代码来建构Transformer模型。\n",
    "\n",
    "### 2.1 Embedding and Positional Encoding\n",
    "\n",
    "编码器与解码器在输入前的资料处理模块，包括 Input Embedding （Output Embedding) 和 Positional Encoding。\n",
    "\n",
    "<div align=center><img src='https://i.imgur.com/4OVf1jD.png' width='400px'/></div>\n",
    "\n",
    "在处理文本之前，需要先对其进行分词(tokenizer)，将文本切分成一个个子字串(token)，每个字串有相对完整的语意，便于后续将token转换为嵌入(Embedding)和位置编码(Positional Encoding)的向量表达。其中 Embedding 可以理解成一个词汇表(lookup table)，有了词汇表后就可以通过查表的方式得到每一个词的词向量。EmbeddingLookup类的主要功能就是创建词汇表，它的核心代码其实只有一行：\n",
    "```python\n",
    "self.embedding_table = Parameter(normal_weight([vocab_size, embedding_size], embedding_size))\n",
    "```\n",
    "词汇表（embedding_table）是一个Parameter类型，说明它里面的值是通过不断迭代学习得到的，效果是使相似的词的词向量有很高的相似度。它的shape为[vocal_size, embed_dim]，其中vocal_size代表所有词汇的总个数，是经过BPE编码得到的；embed_dim是指每一个词用多少维度的向量表示，本案例中将它的值设置为1024。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore import nn\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.common.tensor import Tensor\n",
    "from mindspore.common.parameter import Parameter\n",
    "\n",
    "class EmbeddingLookup(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_dim):\n",
    "        super(EmbeddingLookup, self).__init__()\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        init_weight = np.random.normal(0, embed_dim ** -0.5, size=[vocab_size, embed_dim]).astype(np.float32)\n",
    "        # 0 is Padding index, thus init it as 0.\n",
    "        init_weight[0, :] = 0\n",
    "        # 核心代码\n",
    "        self.embedding_table = Parameter(Tensor(init_weight))\n",
    "        \n",
    "        self.expand = P.ExpandDims()\n",
    "        self.gather = P.Gather()\n",
    "        self.one_hot = P.OneHot()\n",
    "        self.on_value = Tensor(1.0, mstype.float32)\n",
    "        self.off_value = Tensor(0.0, mstype.float32)\n",
    "        self.array_mul = P.MatMul()\n",
    "        self.reshape = P.Reshape()\n",
    "        self.get_shape = P.Shape()\n",
    "\n",
    "    def construct(self, input_ids):\n",
    "        # input_ids (Tensor): A batch of sentences with shape (N, T).\n",
    "        _shape = self.get_shape(input_ids)  # (N, T).\n",
    "        _batch_size = _shape[0]\n",
    "        _max_len = _shape[1]\n",
    "\n",
    "        flat_ids = self.reshape(input_ids, (_batch_size * _max_len,))\n",
    "        output_for_reshape = self.gather(self.embedding_table, flat_ids, 0)\n",
    "\n",
    "        output = self.reshape(output_for_reshape, (_batch_size, _max_len, self.embedding_dim))\n",
    "        # Returns: Tensor, word embeddings with shape (N, T, D)\n",
    "        return output, self.embedding_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面提到，Transformr不包含RNN和CNN结构，而是使用Attention 机制，其忽略了输入序列中每一个元素的位置及元素之间的距离，这样更容易找出序列中每一个元素的关连性，这是它的优点。然而在实际 NLP 应用中，句子内单词的顺序有着相当大的语法及语义上的含义，是不能忽略的。\n",
    "Transformer为了弥补这一缺点，添加了一个“位置编码 (Positional Encoding)”机制：输入序列在经过 embedding 处理后，每一个位置的输入值，都要加上该位置的位置编码，如此便可以在输入序列中加入位置信息。\n",
    "\n",
    "Position Encoding可以使用训练学习，也可以使用固定公式计算。Transformer采用的是不同频率的正弦和余弦函数：\n",
    "$$\n",
    "PE_{pos,2i} = sin(pos/10000^{2i/d_{\\mathrm{model}}}) \\\\\n",
    "PE_{pos,2i+1} = cos(pos/10000^{2i/d_{\\mathrm{model}}})\n",
    "$$\n",
    "其中，$pos$ 指的是这个 word 在这个句子中的位置，$i$ 指的是 embedding 维度($i ∈ {0, 1, 2, 3, …, d_{\\mathrm model}}$，它在偶数位置使用sin函数编码，在奇数位置使用cos函数编码)，$d_{\\mathrm model}$ 为模型的 embedding 大小。最后将其与Word Embedding相加后得到的结果就是编码器或解码器的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(length, depth,\n",
    "                      min_timescale=1,\n",
    "                      max_timescale=1e4):\n",
    "    \"\"\"Create Tensor of sinusoids of different frequencies.\"\"\"\n",
    "    depth = depth // 2\n",
    "    positions = np.arange(length, dtype=np.float32)\n",
    "    log_timescale_increment = (np.log(max_timescale / min_timescale) / (depth - 1))\n",
    "    inv_timescales = min_timescale * np.exp(\n",
    "        np.arange(depth, dtype=np.float32) * -log_timescale_increment)\n",
    "    scaled_time = np.expand_dims(positions, 1) * np.expand_dims(inv_timescales, 0)\n",
    "    x = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Cell):\n",
    "    \"\"\"Add positional info to word embeddings.\"\"\"\n",
    "    def __init__(self,\n",
    "                 embedding_size,\n",
    "                 max_position_embeddings=512):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.add = P.Add()\n",
    "        self.expand_dims = P.ExpandDims()\n",
    "        self.position_embedding_table = Tensor(\n",
    "            position_encoding(max_position_embeddings, embedding_size),\n",
    "            mstype.float32\n",
    "        )\n",
    "        self.gather = P.Gather()\n",
    "        self.get_shape = P.Shape()\n",
    "\n",
    "    def construct(self, word_embeddings):\n",
    "        input_shape = self.get_shape(word_embeddings)\n",
    "        input_len = input_shape[1]\n",
    "        # 核心代码\n",
    "        position_embeddings = self.position_embedding_table[0:input_len:1, ::]\n",
    "        position_embeddings = self.expand_dims(position_embeddings, 0)\n",
    "        output = self.add(word_embeddings, position_embeddings)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 注意力机制\n",
    "\n",
    "Transformer 所用的 Attention 机制，是使用基于「Self-Attention」的「Scaled Dot-Product Attention」，并在同一个子层中，使用 8 个平行的 「Scaled Dot-Product Attention」，成为「Multi-Head」Attention 结构。\n",
    "\n",
    "<div align=center>\n",
    "   <img src='https://i.imgur.com/99YUvcZ.png'/ width=500>\n",
    "</div>\n",
    "\n",
    "输入序列Q/K/V，对于编码器的第一层而言，它们是经过 Embedding 和位置编码的的输入序列，对于编码器的第二层以上的各层而言，它们是前一层的输出。\n",
    "\n",
    "在编码器中，计算单头注意力的方式如下：<br>\n",
    "输入 $[x_1, x_2, x_3, x_4...]$(其中 $x_i$是一个Word Embedding)，每一个 Scaled Dot-Product Attention 有一组属于它自己的权重矩阵 ($W_Q, W_K, W_V$)，输入 $x_i$ 和 $W_Q$ 作矩阵相乘，即得 $q_i$，以此类推，算出各个输入对应的 $q/k/v$，设置Query, key, Value三个矩阵。\n",
    "\n",
    "<div align=center>\n",
    "   <img src='https://i.imgur.com/AF6zqj5.png'/ width=500>\n",
    "</div>\n",
    "\n",
    "透过这三个矩阵计算注意力分数，执行步骤为：<br>\n",
    "第一步 (MatMul)：将 Q 和序列中所有的 K 作内积 (Dot-Product) ，得到一个内积序列。\n",
    "\n",
    "- 例如 $q_1$与 $k_1, k_2, k_3, k_4$ 做点积运算，则输出为 $[a_{1,1}, a_{1,2}, a_{1,3}, a_{1,4}]$， $a_{i, j}$ 表示第 $j$ 个Word Embedding对第 $i$ 个的重要性。\n",
    "\n",
    "第二步 (Scale)：将内积序列中的每个数除以「Ｋ长度的平方根」。<br>\n",
    "第三步 (SoftMax)：将序列做 SoftMax 运算。<br>\n",
    "第四步 (MatMul)：将输入 V 序列中的值，乘上其对应的 SoftMax 结果，得到一个 weighted V 序列。<br>\n",
    "第五步 (Sum)：将 weighted V 序列所有元素相加，得到输出的结果。\n",
    "\n",
    "#### 2.2.1 MultiHeadAttention\n",
    "\n",
    "多头自注意力的计算方式与单头自注意力相同，只是 $W_q, W_k, W_v$ 会重复head_num次。\n",
    "\n",
    "本案例中 $W_q = W_k = W_v =$ attn_embed_dim // num_attn_heads，attn_embed_dim设置为1024，num_attn_heads设置为8。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 src_dim,\n",
    "                 tgt_dim,\n",
    "                 attn_embed_dim,\n",
    "                 num_attn_heads=1,\n",
    "                 query_act=None,\n",
    "                 key_act=None,\n",
    "                 value_act=None,\n",
    "                 out_act=None,\n",
    "                 has_attention_mask=True,\n",
    "                 attention_dropout_prob=0.0,\n",
    "                 initializer_range=0.02,\n",
    "                 do_return_2d_tensor=True,\n",
    "                 compute_type=mstype.float32):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.attn_embed_dim = attn_embed_dim\n",
    "        self.num_attn_heads = num_attn_heads\n",
    "        self.size_per_head = attn_embed_dim // num_attn_heads\n",
    "        self.src_dim = src_dim\n",
    "        self.tgt_dim = tgt_dim\n",
    "        self.has_attention_mask = has_attention_mask\n",
    "        \n",
    "        self.scores_mul = Tensor([1.0 / math.sqrt(float(self.size_per_head))],\n",
    "                                 dtype=compute_type)\n",
    "        self.reshape = P.Reshape()\n",
    "        # 首先设置Q、K、V矩阵，它们的值是通过不断迭代学习到的, shape为[units, from_tensor_width]。代码中设置它的shape为[1024, 64]。\n",
    "        self.query_layer = nn.Dense(src_dim,\n",
    "                                    attn_embed_dim,\n",
    "                                    activation=query_act,\n",
    "                                    has_bias=True,\n",
    "                                    weight_init=TruncatedNormal(initializer_range)).to_float(compute_type)\n",
    "        self.key_layer = nn.Dense(tgt_dim,\n",
    "                                  attn_embed_dim,\n",
    "                                  activation=key_act,\n",
    "                                  has_bias=True,\n",
    "                                  weight_init=TruncatedNormal(initializer_range)).to_float(compute_type)\n",
    "        self.value_layer = nn.Dense(tgt_dim,\n",
    "                                    attn_embed_dim,\n",
    "                                    activation=value_act,\n",
    "                                    has_bias=True,\n",
    "                                    weight_init=TruncatedNormal(initializer_range)).to_float(compute_type)\n",
    "\n",
    "        self.out_layer = nn.Dense(attn_embed_dim,\n",
    "                                  attn_embed_dim,\n",
    "                                  activation=out_act,\n",
    "                                  has_bias=True,\n",
    "                                  weight_init=TruncatedNormal(initializer_range)).to_float(compute_type)\n",
    "\n",
    "        self.matmul_trans_b = P.BatchMatMul(transpose_b=True)\n",
    "        self.multiply = P.Mul()\n",
    "        self.transpose = P.Transpose()\n",
    "        self.multiply_data = Tensor([-10000.0], dtype=compute_type)\n",
    "        self.matmul = P.BatchMatMul()\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(1.0 - attention_dropout_prob)\n",
    "\n",
    "        if self.has_attention_mask:\n",
    "            self.expand_dims = P.ExpandDims()\n",
    "            self.sub = P.Sub()\n",
    "            self.add = P.Add()\n",
    "            self.cast = P.Cast()\n",
    "            self.get_dtype = P.DType()\n",
    "\n",
    "        self.do_return_2d_tensor = do_return_2d_tensor\n",
    "        self.cast_compute_type = SaturateCast(dst_type=compute_type)\n",
    "        self.softmax_cast = P.Cast()\n",
    "        self.get_shape = P.Shape()\n",
    "        self.transpose_orders = (0, 2, 1, 3)\n",
    "\n",
    "    def construct(self, queries, keys, values, attention_mask):\n",
    "        q_shape = self.get_shape(queries)  # (N, T, D)\n",
    "        batch_size = q_shape[0]\n",
    "        src_max_len = q_shape[1]\n",
    "\n",
    "        k_shape = self.get_shape(keys)  # (N, T', D)\n",
    "        tgt_max_len = k_shape[1]\n",
    "\n",
    "        _src_4d_shape = (batch_size, src_max_len, self.num_attn_heads, self.size_per_head)\n",
    "        _tgt_4d_shape = (batch_size, tgt_max_len, self.num_attn_heads, self.size_per_head)\n",
    "\n",
    "        queries_2d = self.reshape(queries, (-1, self.src_dim))\n",
    "        keys_2d = self.reshape(keys, (-1, self.tgt_dim))\n",
    "        values_2d = self.reshape(values, (-1, self.tgt_dim))\n",
    "\n",
    "        query_out = self.query_layer(queries_2d)  # (N*T, D)*(D, D) -> (N*T, D)\n",
    "        key_out = self.key_layer(keys_2d)  # (N*T, D)*(D, D) -> (N*T, D)\n",
    "        value_out = self.value_layer(values_2d)  # (N*T, D)*(D, D) -> (N*T, D)\n",
    "\n",
    "        query_out = self.multiply(query_out, self.scores_mul)\n",
    "\n",
    "        query_layer = self.reshape(query_out, _src_4d_shape)\n",
    "        query_layer = self.transpose(query_layer, self.transpose_orders)  # (N, h, T, D')\n",
    "        key_layer = self.reshape(key_out, _tgt_4d_shape)\n",
    "        key_layer = self.transpose(key_layer, self.transpose_orders)  # (N, h, T', D')\n",
    "        value_layer = self.reshape(value_out, _tgt_4d_shape)\n",
    "        value_layer = self.transpose(value_layer, self.transpose_orders)  # (N, h, T', D')\n",
    "\n",
    "        attention_scores = self.matmul_trans_b(query_layer, key_layer)\n",
    "        # 编码器中不需要做Mask\n",
    "        if self.has_attention_mask:\n",
    "            attention_mask = self.expand_dims(attention_mask, 1)\n",
    "            multiply_out = self.sub(\n",
    "                self.cast(F.tuple_to_array((1.0,)), self.get_dtype(attention_scores)),\n",
    "                self.cast(attention_mask, self.get_dtype(attention_scores))\n",
    "            )  # make mask position into 1, unmask position into 0.\n",
    "            adder = self.multiply(multiply_out, self.multiply_data)\n",
    "            adder = self.softmax_cast(adder, mstype.float32)\n",
    "            attention_scores = self.softmax_cast(attention_scores, mstype.float32)\n",
    "            attention_scores = self.add(adder, attention_scores)\n",
    "\n",
    "        attention_scores = self.softmax_cast(attention_scores, mstype.float32)\n",
    "        attention_prob = self.softmax(attention_scores)\n",
    "        attention_prob = self.softmax_cast(attention_prob, self.get_dtype(key_layer))\n",
    "        attention_prob = self.dropout(attention_prob)\n",
    "        \n",
    "        # (N, h, T, T')*(N, h, T', D') -> (N, h, T, D')\n",
    "        context_layer = self.matmul(attention_prob, value_layer)\n",
    "        context_layer = self.transpose(context_layer, self.transpose_orders)  # (N, T, h, D')\n",
    "        context_layer = self.reshape(context_layer,\n",
    "                                     (batch_size * src_max_len, self.attn_embed_dim))  # (N*T, D)\n",
    "\n",
    "        context_layer = self.out_layer(context_layer)\n",
    "\n",
    "        if not self.do_return_2d_tensor:\n",
    "            context_layer = self.reshape(\n",
    "                context_layer, (batch_size, src_max_len, self.attn_embed_dim)\n",
    "            )  # (N, T, D)\n",
    "\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#### 2.2.2 SelfAttention\n",
    "SelfAttention类中按顺序调用MultiheadAttention类、LayerNorm类(Layer Norm)、ResidualConnection类（Redual Layer)。<br>\n",
    "对应网络框架图中的 MultiheadAttention -> Add & Norm。\n",
    "\n",
    "<div align=center>\n",
    "    <img src='https://i.imgur.com/5KQFzmm.png' width='500px'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 attn_embed_dim,\n",
    "                 num_attn_heads,\n",
    "                 attn_dropout_prob=0.1,\n",
    "                 initializer_range=0.02,\n",
    "                 dropout_prob=0.1,\n",
    "                 has_attention_mask=True,\n",
    "                 compute_type=mstype.float32):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.multi_head_self_attention = MultiHeadAttention(\n",
    "            src_dim=attn_embed_dim,\n",
    "            tgt_dim=attn_embed_dim,\n",
    "            attn_embed_dim=attn_embed_dim,\n",
    "            num_attn_heads=num_attn_heads,\n",
    "            attention_dropout_prob=attn_dropout_prob,\n",
    "            initializer_range=initializer_range,\n",
    "            has_attention_mask=has_attention_mask,\n",
    "            do_return_2d_tensor=False,\n",
    "            compute_type=compute_type)\n",
    "\n",
    "        self.layer_norm = LayerNorm(in_channels=attn_embed_dim)\n",
    "        self.residual = ResidualConnection(dropout_prob=dropout_prob)\n",
    "\n",
    "    def construct(self, queries, keys, values, attention_mask):\n",
    "        q = self.layer_norm(queries)  # (N, T, D)\n",
    "        attention_output = self.multi_head_self_attention(\n",
    "            q, keys, values, attention_mask\n",
    "        )  # (N, T, D)\n",
    "        q = self.residual(attention_output, queries)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.2.3 LayerNorm\n",
    "LayerNorm类实现了Layer Normalization方法，它可以有效的促进网络学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Cell):\n",
    "\n",
    "    def __init__(self, in_channels=None, return_2d=False):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.return_2d = return_2d\n",
    "        self.layer_norm = nn.LayerNorm((in_channels,))\n",
    "        self.cast = P.Cast()\n",
    "        self.get_dtype = P.DType()\n",
    "        self.reshape = P.Reshape()\n",
    "        self.get_shape = P.Shape()\n",
    "\n",
    "    def construct(self, input_tensor):\n",
    "        \"\"\"layer norm\"\"\"\n",
    "        shape = self.get_shape(input_tensor)\n",
    "        batch_size = shape[0]\n",
    "        max_len = shape[1]\n",
    "        embed_dim = shape[2]\n",
    "\n",
    "        output = self.reshape(input_tensor, (-1, embed_dim))\n",
    "        output = self.cast(output, mstype.float32)\n",
    "        output = self.layer_norm(output)\n",
    "        output = self.cast(output, self.get_dtype(input_tensor))\n",
    "        if not self.return_2d:\n",
    "            output = self.reshape(output, (batch_size, max_len, embed_dim))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2.2.4 ResidualConnection\n",
    "ResidualConnection类实现一个残差结构，能有效解决梯度消失的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Cell):\n",
    "\n",
    "    def __init__(self, dropout_prob=0.1):\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        self.add = P.Add()\n",
    "        self.dropout = nn.Dropout(1.0 - dropout_prob)\n",
    "\n",
    "    def construct(self, hidden_tensor, residual):\n",
    "        output = self.dropout(hidden_tensor)\n",
    "        output = self.add(output, residual)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 FeedForward\n",
    "\n",
    "网络结构图中的Feed Forward -> Add & Norm 在FeedForwardNet类中实现。<br>\n",
    "先进行Layer Normalization 然后经过两个全连接层，最后加上残差结构输出。\n",
    "\n",
    "<div align=center>\n",
    "    <img src='https://i.imgur.com/5KQFzmm.png' width='500px'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 hidden_size,\n",
    "                 out_channels,\n",
    "                 hidden_act=\"relu\",\n",
    "                 initializer_range=0.02,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 dropout=None,\n",
    "                 compute_type=mstype.float32):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Dense(in_channels,\n",
    "                            hidden_size,\n",
    "                            activation=hidden_act,\n",
    "                            weight_init=TruncatedNormal(initializer_range)).to_float(compute_type)\n",
    "        self.fc2 = nn.Dense(hidden_size,\n",
    "                            out_channels,\n",
    "                            weight_init=TruncatedNormal(initializer_range)).to_float(compute_type)\n",
    "\n",
    "        self.layer_norm = LayerNorm(in_channels=in_channels,\n",
    "                                    return_2d=True)\n",
    "        self.residual = ResidualConnection(\n",
    "            dropout_prob=hidden_dropout_prob if dropout is None else dropout\n",
    "        )\n",
    "        self.get_shape = P.Shape()\n",
    "        self.reshape = P.Reshape()\n",
    "        self.dropout = nn.Dropout(keep_prob=1.0 - hidden_dropout_prob)\n",
    "\n",
    "    def construct(self, input_tensor):\n",
    "        shape = self.get_shape(input_tensor)\n",
    "        batch_size = shape[0]\n",
    "        max_len = shape[1]\n",
    "        embed_dim = shape[2]\n",
    "        # 核心代码如下\n",
    "        output = self.layer_norm(input_tensor)\n",
    "        output = self.fc1(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)  # (-1, D)\n",
    "        output = self.residual(self.reshape(output, (batch_size, max_len, embed_dim)),\n",
    "                               input_tensor)  # (N, T, D)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### 2.4 编码器与解码器\n",
    "\n",
    "<div align=center><img src='https://i.imgur.com/NNiUhIt.png' width='350px'/></div>\n",
    "\n",
    "可以看到编码器与解码器是由几个模块构成的，分别是Multi-Head Attention、Feed Forward、Masked Multi-Head Attention。\n",
    "\n",
    "#### 2.4.1 编码器\n",
    "\n",
    "编码器包含 6 个完全相同的神经网路层，每一个网路层包含了 2 个子层，第一个子层是 「multi-head self-attention」 层，第二个子层是 「positionwise fully connected feed-forward」 层。在每个层后还有一个正规化与残差运算。\n",
    "\n",
    "用数学式来看，每一个网路层可以表示为：<br>\n",
    "输入: $x$ <br>\n",
    "第一个子层输出: $y=LayerNorm(x+MHA(x))$ <br>\n",
    "第二个子层输出: $O=LayerNorm(y+PFF(y))$ <br>\n",
    "\n",
    "```\n",
    "编码器单层结构如下：\n",
    "    -> pre_LayerNorm\n",
    "    -> Multi-head Self-Attention\n",
    "    -> Dropout & Add\n",
    "    -> pre_LayerNorm\n",
    "    -> Fc1\n",
    "    -> Activation Function\n",
    "    -> Dropout\n",
    "    -> Fc2\n",
    "    -> Dropout & Add\n",
    "```\n",
    "\n",
    "EncoderCell类就是按顺序调用SelfAttention类与FeedForward类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCell(nn.Cell):\n",
    "    \"\"\"Single Encoder layer.\"\"\"\n",
    "    def __init__(self,\n",
    "                 attn_embed_dim=768,\n",
    "                 num_attn_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 attention_dropout_prob=0.02,\n",
    "                 initializer_range=0.02,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 hidden_act=\"relu\",\n",
    "                 compute_type=mstype.float32):\n",
    "        super(EncoderCell, self).__init__()\n",
    "        self.attention = SelfAttention(\n",
    "            attn_embed_dim=attn_embed_dim,\n",
    "            num_attn_heads=num_attn_heads,\n",
    "            attn_dropout_prob=attention_dropout_prob,\n",
    "            initializer_range=initializer_range,\n",
    "            dropout_prob=hidden_dropout_prob,\n",
    "            compute_type=compute_type)\n",
    "        self.feed_forward_net = FeedForwardNet(\n",
    "            in_channels=attn_embed_dim,\n",
    "            hidden_size=intermediate_size,\n",
    "            out_channels=attn_embed_dim,\n",
    "            hidden_act=hidden_act,\n",
    "            initializer_range=initializer_range,\n",
    "            hidden_dropout_prob=hidden_dropout_prob,\n",
    "            dropout=hidden_dropout_prob,\n",
    "            compute_type=compute_type)\n",
    "\n",
    "    def construct(self, queries, attention_mask):\n",
    "        attention_output = self.attention(queries, queries, queries,\n",
    "                                          attention_mask)  # (N, T, D)\n",
    "        output = self.feed_forward_net(attention_output)  # (N, T, D)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransformerEncoder类的功能是重复构造EncoderCell类，然后顺序连接起来，对应网络框架图中的Encoder部分的Nx结构，实际上是重复了6次。\n",
    "\n",
    "<div align=center><img src='https://i.imgur.com/DMbU83g.png' width='400px'/></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 attn_embed_dim,\n",
    "                 encoder_layers,\n",
    "                 num_attn_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 attention_dropout_prob=0.1,\n",
    "                 initializer_range=0.02,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 hidden_act=\"relu\",\n",
    "                 compute_type=mstype.float32):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.num_layers = encoder_layers\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(encoder_layers):\n",
    "            layer = EncoderCell(\n",
    "                attn_embed_dim=attn_embed_dim,\n",
    "                num_attn_heads=num_attn_heads,\n",
    "                intermediate_size=intermediate_size,\n",
    "                attention_dropout_prob=attention_dropout_prob,\n",
    "                initializer_range=initializer_range,\n",
    "                hidden_dropout_prob=hidden_dropout_prob,\n",
    "                hidden_act=hidden_act,\n",
    "                compute_type=compute_type\n",
    "            )\n",
    "            layers.append(layer)\n",
    "        # num_attention_heads的值为8，通过nn.CellList顺序连接起来。\n",
    "        self.layers = nn.CellList(layers)\n",
    "        self.layer_norm = LayerNorm(in_channels=attn_embed_dim)\n",
    "\n",
    "    def construct(self, input_tensor, attention_mask):\n",
    "        prev_output = input_tensor\n",
    "        for layer_module in self.layers:\n",
    "            prev_output = layer_module(prev_output,\n",
    "                                       attention_mask)  # (N, T, D)\n",
    "        prev_output = self.layer_norm(prev_output)  # (N, T, D)\n",
    "        return prev_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 解码器\n",
    "\n",
    "解码器也同样包含了 6 个相同的神经网路层，每一个网路层含有 3 个子层，由下到上分别是：\n",
    "\n",
    ">Masked Multi-Head Self-Attention <br>\n",
    ">Multi-Head Attention <br>\n",
    ">Positionwise Fully Connected Feed-Forward\n",
    "\n",
    "每一个子层也实作残差连接及正规化处理。\n",
    "\n",
    "要使用 Masked Multi-Head Self-Attention 的理由是：确保在计算第 i 个解码器输出时，仅用到 i 之前的输出值，也就是在t时刻只能看到t时刻之前的信息，不不能看到t时刻之后的信息。\n",
    "\n",
    "第二个子层 Multi-Head Attention，是以“第一个子层的输出”和“编码器的输出”执行 Attention 计算。\n",
    "\n",
    "CreateAttentionMaskFromInputMask类实现Mask Self Attention中的Mask功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 案例实现\n",
    "\n",
    "### 3.1 环境建置\n",
    "本案例的开发环境为Modelarts上的notebook。<br>\n",
    "版本：mindspore 1.8.1、python 3.7<br>\n",
    "Ascend 环境：1\\*Ascend 910 | CPU: 24vCPUs 96GB<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindSpore version:  1.8.1\n",
      "The result of multiplication calculation is correct, MindSpore has been installed successfully!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "print(mindspore.run_check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进入开发环境后还需安装一些所需套件，执行以下命令即可："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ma-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若是nltk的punkt套件无法下载的话请至[该连接](https://github.com/nltk/nltk_data/tree/gh-pages/packages/tokenizers)下载punkt.zip，解压后将punkt资料夹放入/root/nltk_data/tokenizers/路径下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 代码结构\n",
    "\n",
    "MASS脚本及代码结构如下：\n",
    "\n",
    "```text\n",
    "├── mass\n",
    "  ├── config\n",
    "  │   ├──config.py                           // 参数配置\n",
    "  ├── src\n",
    "      │   ├──model_utils\n",
    "      │   ├──config.py                       // 参数配置\n",
    "      │   ├──device_adapter.py               // 设备配置\n",
    "      │   ├──local_adapter.py                // 本地设备配置\n",
    "      │   ├──moxing_adapter.py               // modelarts设备配置\n",
    "  ├──src\n",
    "  │   ├──dataset\n",
    "  │      ├──bi_data_loader.py                // 数据集加载器，用于微调或推理\n",
    "  │      ├──mono_data_loader.py              // 预训练数据集加载器\n",
    "  │   ├──language_model\n",
    "  │      ├──noise_channel_language_model.p   // 数据集生成噪声通道语言模型\n",
    "  │      ├──mass_language_model.py           // 基于MASS论文的MASS语言模型\n",
    "  │      ├──loose_masked_language_model.py   // 基于MASS发布代码的MASS语言模型\n",
    "  │      ├──masked_language_model.py         // 基于MASS论文的MASS语言模型\n",
    "  │   ├──transformer\n",
    "  │      ├──create_attn_mask.py              // 生成屏蔽矩阵，除去填充部分\n",
    "  │      ├──transformer.py                   // Transformer模型架构\n",
    "  │      ├──encoder.py                       // Transformer编码器组件\n",
    "  │      ├──decoder.py                       // Transformer解码器组件\n",
    "  │      ├──self_attention.py                // 自注意块组件\n",
    "  │      ├──multi_head_attention.py          // 多头自注意组件\n",
    "  │      ├──embedding.py                     // 嵌入组件\n",
    "  │      ├──positional_embedding.py          // 位置嵌入组件\n",
    "  │      ├──feed_forward_network.py          // 前馈网络\n",
    "  │      ├──residual_conn.py                 // 残留块\n",
    "  │      ├──beam_search.py                   // 推理所用的波束搜索解码器\n",
    "  │      ├──transformer_for_infer.py         // 使用Transformer进行推理\n",
    "  │      ├──transformer_for_train.py         // 使用Transformer进行训练\n",
    "  │   ├──utils\n",
    "  │      ├──byte_pair_encoding.py            // 使用subword-nmt应用字节对编码（BPE）\n",
    "  │      ├──dictionary.py                    // 字典\n",
    "  │      ├──loss_moniter.py                  // 训练步骤中损失监控回调\n",
    "  │      ├──lr_scheduler.py                  // 学习速率调度器\n",
    "  │      ├──ppl_score.py                     // 基于N-gram的困惑度评分\n",
    "  │      ├──rouge_score.py                   // 计算ROUGE得分\n",
    "  │      ├──load_weights.py                  // 从检查点或者NPZ文件加载权重\n",
    "  │      ├──initializer.py                   // 参数初始化器\n",
    "  ├── vocab\n",
    "  │   ├──all.bpe.codes                       // 字节对编码表\n",
    "  │   ├──all_en.dict.bin                     // 已学习到的词汇表\n",
    "  ├── scripts\n",
    "  │   ├──run_ascend.sh                       // Ascend处理器上训练&评估模型脚本\n",
    "  │   ├──run_gpu.sh                          // GPU处理器上训练&评估模型脚本\n",
    "  │   ├──learn_subword.sh                    // 学习字节对编码\n",
    "  │   ├──stop_training.sh                    // 停止训练\n",
    "  ├── requirements.txt                       // 第三方包需求\n",
    "  ├── train.py                               // 训练API入口\n",
    "  ├── eval.py                                // 推理API入口\n",
    "  ├── default_config.yaml                    // 参数配置\n",
    "  ├── tokenize_corpus.py                     // 语料标记化\n",
    "  ├── apply_bpe_encoding.py                  // 应用BPE进行编码\n",
    "  ├── weights_average.py                     // 将各模型检查点平均转换到NPZ格式\n",
    "  ├── news_crawl.py                          // 创建预训练所用的News Crawl数据集\n",
    "  ├── gigaword.py                            // 创建Gigaword语料库\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 准备数据集\n",
    "案例实现中预训练所使用的数据即News Crawl的英语单语数据数据集，因為考量到訓練模型所需時間，只使用2012年的數據(News Crawl: articles from 2012)，下载好的数据集为一纯文字文件，接下来需要对该数据进行预处理，预处理包括对数据进行分词、利用subword-nmt工具生成字节对编码(BPE)、对分词后的语料应用该BPE编码進行切分并构建词彙表幾個步驟。\n",
    "\n",
    "而微调模型用于文本摘要任务所使用的数据集为Gigaword，该数据集已经有分割为训练、测试、验证集，有原文本(src)和目标摘要(tgt)两个文件，本案例只会使用训练集与测试集，也需要對這兩個文件進行同樣的預處理。\n",
    "\n",
    "数据集文件路径结构如下：\n",
    "\n",
    "```text\n",
    ".dataset/<br>\n",
    "└── news_crawl<br>\n",
    "    └── news2012.txt(1.8GB)\n",
    "└── ggw_data<br>\n",
    "    ├── test.src.txt(335KB)\n",
    "    ├── test.tgt.txt(104KB)\n",
    "    ├── train.src.txt(695.2MB)\n",
    "    └── train.tgt.txt(198.5MB)\n",
    "```\n",
    "下载连接：<br>\n",
    "News Crawl数据集（WMT，2019年）：https://www.statmt.org/wmt16/translation-task.html<br>\n",
    "Gigaword语料库（Graff等人，2003年）： https://drive.google.com/open?id=0B6N7tANPyVeBNmlSX19Ld2xDU1E\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 对原始数据进行分词\n",
    "\n",
    "文本是由段落（Paragraph）构成的，段落是由句子（Sentence）构成的，句子是由单词构成的。分词是文本分析的第一步，它把文本段落分解为较小的实体（如单词或句子），每一个实体叫做一个Token。NLTK套件能够实现句子切分和单词切分两种功能。\n",
    "\n",
    "因为原始数据是一行一行句子的格式，所以只要调用nltk套件中的word_tokenize()函数文本进行分词，此函数的作用是基于空格/标点等对文本进行分词，返回分词后的列表，最后只保留长度大于175的单词列表，并将这些单词列表写入目标文件中。\n",
    "\n",
    "```\n",
    "sen = \"Dave Aneckstein, Simmons Research, an Experian Company\"\n",
    "tokens = word_tokenize(sen)\n",
    "tokens = ['Dave', 'Aneckstein', ',', 'Simmons', 'Research', ',', 'an', 'Experian', 'Company']\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Processing ./dataset/news_crawl/news2007.txt.\n",
      " | Wrote to ./tokenized_corpus/news_crawl/news2007_tokenized.txt.\n",
      " | Processing ./dataset/ggw_data/test.src.txt.\n",
      " | Wrote to ./tokenized_corpus/ggw_data/test.src_tokenized.txt.\n",
      " | Processing ./dataset/ggw_data/train.src.txt.\n",
      " | Wrote to ./tokenized_corpus/ggw_data/train.src_tokenized.txt.\n",
      " | Processing ./dataset/ggw_data/train.tgt.txt.\n",
      " | Wrote to ./tokenized_corpus/ggw_data/train.tgt_tokenized.txt.\n",
      " | Processing ./dataset/ggw_data/test.tgt.txt.\n",
      " | Wrote to ./tokenized_corpus/ggw_data/test.tgt_tokenized.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data_list = ['news_crawl', 'ggw_data']\n",
    "src_folder = \"./dataset/\"\n",
    "out_folder = \"./tokenized_corpus/\"\n",
    "\n",
    "if not os.path.isdir(out_folder):\n",
    "    os.mkdir(out_folder)\n",
    "if not os.path.isdir(out_folder+'news_crawl/'):\n",
    "    os.mkdir(out_folder+'news_crawl/')\n",
    "if not os.path.isdir(out_folder+'ggw_data/'):\n",
    "    os.mkdir(out_folder+'ggw_data/')\n",
    "\n",
    "def create_tokenized_sentences(file_path, tokenized_file):\n",
    "    tokenized_sen = []\n",
    "    print(f\" | Processing {file_path}.\")\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for sen in file:\n",
    "            tokens = word_tokenize(sen)\n",
    "            tokens = [t for t in tokens if t != \" \"]\n",
    "            if len(tokens) > 175:\n",
    "                continue\n",
    "            tokenized_sen.append(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "    with open(tokenized_file, \"w\") as file:\n",
    "        file.writelines(tokenized_sen)\n",
    "    print(f\" | Wrote to {tokenized_file}.\")\n",
    "\n",
    "for item in data_list:\n",
    "    folder_path = os.path.join(src_folder, item)\n",
    "    output_path = os.path.join(out_folder, item)\n",
    "    for file in os.listdir(folder_path):\n",
    "        if not file.endswith(\".txt\"):\n",
    "            continue\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        tokenized_file = os.path.join(output_path, file.replace(\".txt\", \"_tokenized.txt\"))\n",
    "        create_tokenized_sentences(file_path, tokenized_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 生成字节对编码\n",
    "虽然我们已经将数据进行了简单的分词，但是实际上这样的分词是不够细緻的，若要使用一个包含所有单词的字典，将需要很大的空间和计算量，而且过大的token列表也会影响模型的预测准确度。\n",
    "\n",
    "字节对编码（BPE, Byte Pair Encoder）是一种简单的数据压缩算法，它将字符串中出现频率最高的相邻字符替换成一个不存在的新字符，反复进行该操作直到满足某些预设条件为止（字符表大小、迭代次数）。后续使用时还需要一个词表来重建原始数据。\n",
    "\n",
    "BPE的处理过程可以理解为一个单词的再拆分过程。如\"loved\",\"loving\",\"loves\"这三个单词，其本身的语义都是”爱”的意思。BPE通过训练，能够把上面的3个单词拆分成”lov”,”ed”,”ing”,”es”几部分，这样可以把词的本身的意思和时态分开，有效的减少了词表大小，且罕见的词会被分解为两个或多个subword tokens，能比较好的处理OOV(out of vocabulary)问题。\n",
    "\n",
    "我们可以利用subword-nmt工具学习文本的字节对编码(learn-bpe)：\n",
    "\n",
    "先设定选择最高频字节对的次数num_operations，本案例设定为46000。此过程会统计每一个相邻字节对的出现频率，并保存为code_file。\n",
    "```\n",
    "subword-nmt learn-bpe -i {input_src_file} -s {num_operations} -o {codes_file}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./dataset/ggw_data/*.txt ./dataset/news_crawl/*.txt | subword-nmt learn-bpe -s 46000 -o ./dataset/all.bpe.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 应用字节对编码并构建词彙表\n",
    "使用上一节获取的BPE编码对分词后的语料进行字节对编码处理(apply-bpe)，生成subword词表。此过程会将input_tokenized_file中的单词拆分为字符序列并在末尾添加后缀“<\\/w>”，而后按照code_file将出现频率最高的字节对合并成新的subword，重複合併直到达到设定的subword词表大小或下一个最高频的字节对出现频率为1，最后将结果保存为out_file，文件名为input_tokenized_file_bpe.txt。\n",
    "```\n",
    "subword-nmt apply-bpe -c {code_file} < {input_tokenized_file} > {out_file}\n",
    "可以通过 --vocabulary-threshold {threshold} 選項过滤词频低于阈值的单词来缩小词汇量。\n",
    "```\n",
    "构建词彙表(get-vocab)的具体作法为：得到subword词表后，对该词表按照子词长度由大到小排序。编码时，对于每个单词，遍历词表寻找是否有token是当前单词的子字符串，如果有，则该token为表示当前单词的tokens之一。从最长的token迭代到最短的token，尝试将每个单词中的子字符串替换为已存在的token。若每个单词都替换完后仍然有子字符串没被替换，则将剩余的子词替换为特殊token，如\\<unk>。\n",
    "```\n",
    "subword-nmt get-vocab -i {output_path} -o {dict_path}\n",
    "# output_path為apply-bpe的輸出路徑\n",
    "# dict_path為輸出的.dict檔案路徑\n",
    "```\n",
    "\n",
    "根据News Crawl 2012数据集的14869673个句子，学习到的词汇量为39284个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Applying BPE encoding.\n",
      " | Fetching vocabulary from single file.\n",
      " | Vocabulary Size: 37203\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from src.utils import Dictionary\n",
    "\n",
    "source_folder = os.path.abspath(\"./tokenized_corpus/news_crawl/\")\n",
    "output_folder = os.path.abspath(\"./tokenized_corpus/news_crawl/bpe/\")\n",
    "codes = os.path.abspath(\"./dataset/all.bpe.codes\")\n",
    "vocab_path = \"./vocab/all_en.dict.bin\"\n",
    "\n",
    "if not os.path.isdir(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "if not os.path.isdir('./vocab/'):\n",
    "    os.mkdir('./vocab/')\n",
    "\n",
    "\n",
    "ENCODER = \"subword-nmt apply-bpe -c\"\n",
    "LEARN_DICT = \"subword-nmt get-vocab -i\"\n",
    "def bpe_encode(codes_path, src_path, output_path, dict_path):\n",
    "    # Encoding.\n",
    "    print(\" | Applying BPE encoding.\")\n",
    "    commands = ENCODER.split() + [codes_path] + [\"-i\"] + [src_path] + [\"-o\"] + [output_path]\n",
    "    subprocess.call(commands)\n",
    "    print(\" | Fetching vocabulary from single file.\")\n",
    "    # Learn vocab.\n",
    "    commands = LEARN_DICT.split() + [output_path] + [\"-o\"] + [dict_path]\n",
    "    subprocess.call(commands)\n",
    "\n",
    "available_dict = []\n",
    "for file in os.listdir(source_folder):\n",
    "    if file.endswith(\".txt\"):\n",
    "        output_path = os.path.join(output_folder, file.replace(\".txt\", \"_bpe.txt\"))\n",
    "        dict_path = os.path.join(output_folder, file.replace(\".txt\", \".dict\"))\n",
    "        available_dict.append(dict_path)\n",
    "        bpe_encode(codes, os.path.join(source_folder, file), output_path, dict_path)\n",
    "        \n",
    "# 加载bpe_encode處理過的文本词汇表，行格式为[word, freq]。\n",
    "vocab = Dictionary.load_from_text(available_dict)\n",
    "vocab.persistence(vocab_path) #将词汇表对象保存为二进制文件。\n",
    "print(f\" | Vocabulary Size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 生成数据集\n",
    "\n",
    "因为生成的数据集格式是tfrecord, 这边需要将kernal修改为tensorflow-1.15，再安装mindspore。\n",
    "并使用以下命令处理下某些套件与tensorflow的冲突。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://192.168.0.122:8888/repository/pypi/simple\n",
      "Collecting mindspore-ascend==1.8.1\n",
      "  Downloading http://192.168.0.122:8888/repository/pypi/packages/mindspore-ascend/1.8.1/mindspore_ascend-1.8.1-cp37-none-any.whl (129.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 129.0 MB 67.2 MB/s eta 0:00:01|████▎                           | 17.2 MB 48.9 MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting rouge\n",
      "  Downloading http://192.168.0.122:8888/repository/pypi/packages/rouge/1.0.1/rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Collecting scipy>=1.5.2\n",
      "  Downloading http://192.168.0.122:8888/repository/pypi/packages/scipy/1.7.3/scipy-1.7.3-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (36.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 36.1 MB 48.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.3\n",
      "  Downloading http://192.168.0.122:8888/repository/pypi/packages/astunparse/1.6.3/astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages (from mindspore-ascend==1.8.1) (7.0.0)\n",
      "Collecting protobuf>=3.13.0\n",
      "  Downloading http://192.168.0.122:8888/repository/pypi/packages/protobuf/4.21.9/protobuf-4.21.9-cp37-abi3-manylinux2014_aarch64.whl (403 kB)\n",
      "\u001b[K     |████████████████████████████████| 403 kB 44.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.6.1 in /home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages (from mindspore-ascend==1.8.1) (5.7.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages (from mindspore-ascend==1.8.1) (1.17.5)\n",
      "Collecting asttokens>=2.0.0\n",
      "  Downloading http://192.168.0.122:8888/repository/pypi/packages/asttokens/2.1.0/asttokens-2.1.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading http://192.168.0.122:8888/repository/pypi/packages/packaging/21.3/packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 40.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages (from rouge) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages (from astunparse>=1.3->mindspore-ascend==1.8.1) (0.37.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages (from packaging>=20.0->mindspore-ascend==1.8.1) (3.0.6)\n",
      "Installing collected packages: scipy, protobuf, packaging, astunparse, asttokens, rouge, mindspore-ascend\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.3.3\n",
      "    Uninstalling scipy-1.3.3:\n",
      "      Successfully uninstalled scipy-1.3.3\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.11.3\n",
      "    Uninstalling protobuf-3.11.3:\n",
      "      Successfully uninstalled protobuf-3.11.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 1.15.0 requires gast==0.2.2, but you have gast 0.5.2 which is incompatible.\n",
      "moxing-framework 2.0.0.rc2.4b57a67b requires urllib3>=1.26.2, but you have urllib3 1.21.1 which is incompatible.\u001b[0m\n",
      "Successfully installed asttokens-2.1.0 astunparse-1.6.3 mindspore-ascend-1.8.1 packaging-21.3 protobuf-4.21.9 rouge-1.0.1 scipy-1.7.3\n",
      "Found existing installation: urllib3 1.21.1\n",
      "Uninstalling urllib3-1.21.1:\n",
      "  Successfully uninstalled urllib3-1.21.1\n",
      "Found existing installation: chardet 4.0.0\n",
      "Uninstalling chardet-4.0.0:\n",
      "  Successfully uninstalled chardet-4.0.0\n",
      "Looking in indexes: http://192.168.0.122:8888/repository/pypi/simple\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ma-user/modelarts-dev/modelarts-sdk (from requests) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/TensorFlow-1.15.0/lib/python3.7/site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ma-user/modelarts-dev/common-algo-toolkit (from requests) (1.26.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install mindspore-ascend==1.8.1 rouge\n",
    "!pip uninstall -y urllib3 chardet\n",
    "!pip install --upgrade requests\n",
    "# !pip uninstall -y mindspore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Processing corpus ./dataset/news_crawl/news2007.txt.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create News Crawl Pre-Training Dataset.\"\"\"\n",
    "import os\n",
    "from src.dataset import MonoLingualDataLoader\n",
    "from src.language_model import LooseMaskedLanguageModel\n",
    "from src.utils import Dictionary\n",
    "\n",
    "input_folder_path = './dataset/news_crawl/' # Raw corpus folder\n",
    "output_folder_path = './train_data/news_crawl/' # Dataset output path\n",
    "\n",
    "if not os.path.isdir(output_folder_path):\n",
    "    os.mkdir('./train_data')\n",
    "    os.mkdir('./train_data/news_crawl/')\n",
    "\n",
    "vocab_path = './vocab/all_en.dict.bin' # Existed vocab path\n",
    "vocab = Dictionary.load_from_persisted_dict(vocab_path)\n",
    "\n",
    "def create_pre_train(text_file, output_folder, vocab, max_sen_len):\n",
    "    \n",
    "    loader = MonoLingualDataLoader(\n",
    "        src_filepath=text_file,\n",
    "        lang=\"en\", dictionary=vocab,\n",
    "        language_model=LooseMaskedLanguageModel(mask_ratio=0.5, mask_all_prob=None),\n",
    "        max_sen_len=max_sen_len, min_sen_len=10\n",
    "    )\n",
    "\n",
    "    src_file_name = os.path.basename(text_file)\n",
    "\n",
    "    file_name = os.path.join(\n",
    "        output_folder_path,\n",
    "        src_file_name.replace('.txt', f'_len_{max_sen_len}.tfrecord')\n",
    "    )\n",
    "    \n",
    "    loader.write_to_tfrecord(path=file_name)\n",
    "\n",
    "for file in os.listdir(input_folder_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        create_pre_train(os.path.join(input_folder_path, file),output_folder_path, vocab, 32)\n",
    "\n",
    "print(f\" | Generate Dataset for Pre-training is done.\")\n",
    "print(f\" | Vocabulary size: {vocab.size}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Generate Gigaword dataset.\"\"\"\n",
    "import os\n",
    "from src.dataset import BiLingualDataLoader\n",
    "from src.language_model import NoiseChannelLanguageModel\n",
    "from src.utils import Dictionary\n",
    "\n",
    "input_folder_path = './dataset/ggw_data/'\n",
    "output_folder_path = './train_data/ggw_data/'\n",
    "\n",
    "if not os.path.isdir(output_folder_path):\n",
    "    os.mkdir('./train_data/ggw_data/')\n",
    "\n",
    "vocab_path = './vocab/all_en.dict.bin'\n",
    "vocab = Dictionary.load_from_persisted_dict(vocab_path)\n",
    "\n",
    "train = BiLingualDataLoader(\n",
    "    src_filepath=os.path.join(input_folder_path,\"train.src.txt\"),\n",
    "    tgt_filepath=os.path.join(input_folder_path,\"train.tgt.txt\"),\n",
    "    src_dict=vocab, tgt_dict=vocab,\n",
    "    src_lang=\"en\", tgt_lang=\"en\",\n",
    "    language_model=NoiseChannelLanguageModel(add_noise_prob=0.),\n",
    "    max_sen_len=32\n",
    ")\n",
    "\n",
    "train.write_to_tfrecord(\n",
    "    path=os.path.join(output_folder_path, \"gigaword_train_dataset.tfrecord\")\n",
    ")\n",
    "\n",
    "test = BiLingualDataLoader(\n",
    "    src_filepath=os.path.join(input_folder_path,\"test.src.txt\"),\n",
    "    tgt_filepath=os.path.join(input_folder_path,\"test.tgt.txt\"),\n",
    "    src_dict=vocab, tgt_dict=vocab,\n",
    "    src_lang=\"en\", tgt_lang=\"en\",\n",
    "    language_model=NoiseChannelLanguageModel(add_noise_prob=0),\n",
    "    max_sen_len=32\n",
    ")\n",
    "\n",
    "test.write_to_tfrecord(\n",
    "    path=os.path.join(output_folder_path, \"gigaword_test_dataset.tfrecord\")\n",
    ")\n",
    "\n",
    "print(f\" | Generate Dataset for fine-tuneing is done.\")\n",
    "print(f\" | Vocabulary size: {vocab.size}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.4 预训练\n",
    "\n",
    "预训练中，采用Adam优化器和损失放大来得到预训练后的模型。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"定义训练与推理的参数\"\"\"\n",
    "import mindspore.common.dtype as mstype\n",
    "class config():\n",
    "    enable_modelarts = True #Whether training on modelarts, default = False\n",
    "    device_target = \"Ascend\"\n",
    "    output_path = \"./output/\"\n",
    "    save_checkpoint_path = \"./output/checkpoint/\"\n",
    "    checkpoint_file_path = \"\"\n",
    "    # Training options\n",
    "    epochs = 20\n",
    "    batch_size = 192\n",
    "    dtype = mstype.float32 #only support float16 and float32\n",
    "    compute_type = mstype.float16 #only support float16 and float32\n",
    "    pre_train_dataset = \"./train_data/news_crawl/news2012_len_32.tfrecord-001-of-001\"\n",
    "    fine_tune_dataset = \"./train_data/ggw_data/gigaword_train_dataset.tfrecord-001-of-001\"\n",
    "    test_dataset = \"./train_data/ggw_data/gigaword_test_dataset.tfrecord-001-of-001\"\n",
    "    dataset_sink_mode = False\n",
    "    dataset_sink_step = 100\n",
    "    random_seed = 100\n",
    "    save_graphs = False\n",
    "    seq_length = 32 #64\n",
    "    vocab_size = 39284\n",
    "    hidden_size = 1024\n",
    "    num_hidden_layers = 6\n",
    "    num_attention_heads = 8\n",
    "    intermediate_size = 4096\n",
    "    hidden_act = \"relu\"\n",
    "    hidden_dropout_prob = 0.2\n",
    "    attention_dropout_prob = 0.2\n",
    "    max_position_embeddings = 32 #64\n",
    "    initializer_range = 0.02\n",
    "    label_smoothing = 0.1\n",
    "    beam_width = 4\n",
    "    length_penalty_weight = 1.0\n",
    "    max_decode_length = 32 #64\n",
    "    init_loss_scale = 65536\n",
    "    loss_scale_factor = 2\n",
    "    scale_window = 200\n",
    "    lr = 0.0001\n",
    "    poly_lr_scheduler_power = 0.5\n",
    "    decay_steps = 10000\n",
    "    decay_start_step = 12000\n",
    "    warmup_steps = 4000\n",
    "    min_lr = 0.000001\n",
    "    save_ckpt_steps = 10000\n",
    "    keep_ckpt_max = 50\n",
    "    ckpt_prefix = \"pt\"\n",
    "    metric = \"rouge\"\n",
    "    vocab = \"./vocab/all_en.dict.bin\"\n",
    "    output = \"./output/infer.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from mindspore.common.tensor import Tensor\n",
    "from mindspore.nn import Momentum\n",
    "from mindspore.nn.optim import Adam, Lamb\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.train.loss_scale_manager import DynamicLossScaleManager, FixedLossScaleManager\n",
    "from mindspore.train.callback import CheckpointConfig, ModelCheckpoint, TimeMonitor\n",
    "from mindspore import context, Parameter\n",
    "from mindspore.communication import management as MultiAscend\n",
    "from mindspore.train.serialization import load_checkpoint\n",
    "from mindspore.common import set_seed\n",
    "\n",
    "from src.dataset import load_dataset\n",
    "from src.transformer import TransformerNetworkWithLoss, TransformerTrainOneStepWithLossScaleCell\n",
    "from src.utils import LossCallBack\n",
    "from src.utils import one_weight, zero_weight, weight_variable\n",
    "from src.utils import square_root_schedule\n",
    "from src.utils.lr_scheduler import polynomial_decay_scheduler, BertLearningRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config()\n",
    "config.epochs = 1 #for test\n",
    "\n",
    "print(\" | Starting training on single device.\")\n",
    "pre_train_dataset = load_dataset(data_files=config.pre_train_dataset,\n",
    "                                 batch_size=config.batch_size,\n",
    "                                 epoch_count=1,\n",
    "                                 sink_mode=config.dataset_sink_mode,\n",
    "                                 sink_step=config.dataset_sink_step)\n",
    "\n",
    "# 定义帶loss的網路\n",
    "net_with_loss = TransformerNetworkWithLoss(config, is_training=True)\n",
    "net_with_loss.init_parameters_data()\n",
    "\n",
    "for param in net_with_loss.trainable_params():\n",
    "    name = param.name\n",
    "    value = param.data\n",
    "    if isinstance(value, Tensor):\n",
    "        if name.endswith(\".gamma\"):\n",
    "            param.set_data(one_weight(value.asnumpy().shape))\n",
    "        elif name.endswith(\".beta\") or name.endswith(\".bias\"):\n",
    "            param.set_data(zero_weight(value.asnumpy().shape))\n",
    "        else:\n",
    "            param.set_data(weight_variable(value.asnumpy().shape))\n",
    "\n",
    "update_steps = config.epochs * pre_train_dataset.get_dataset_size()\n",
    "\n",
    "# 定义递减的学习率\n",
    "lr = Tensor(polynomial_decay_scheduler(lr=config.lr,\n",
    "           min_lr=config.min_lr,\n",
    "           decay_steps=config.decay_steps,\n",
    "           total_update_num=update_steps,\n",
    "           warmup_steps=config.warmup_steps,\n",
    "           power=config.poly_lr_scheduler_power), dtype=mstype.float32)\n",
    "# 定义优化器\n",
    "optimizer = Adam(net_with_loss.trainable_params(), lr, beta1=0.9, beta2=0.98)\n",
    "\n",
    "# loss scale (mode = dynamic)\n",
    "scale_manager = DynamicLossScaleManager(init_loss_scale=config.init_loss_scale,\n",
    "                                        scale_factor=config.loss_scale_factor,\n",
    "                                        scale_window=config.scale_window)\n",
    "# 定义反向网络\n",
    "net_with_grads = TransformerTrainOneStepWithLossScaleCell(network=net_with_loss, optimizer=optimizer,\n",
    "                                                          scale_update_cell=scale_manager.get_update_cell())\n",
    "net_with_grads.set_train(True)\n",
    "\n",
    "# 初始化模型\n",
    "model = Model(net_with_grads)\n",
    "\n",
    "time_cb = TimeMonitor(data_size=pre_train_dataset.get_dataset_size())\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=config.save_ckpt_steps,\n",
    "                               keep_checkpoint_max=config.keep_ckpt_max)\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(time_cb)\n",
    "ckpt_save_dir = config.save_checkpoint_path\n",
    "\n",
    "ckpt_callback = ModelCheckpoint(\n",
    "    prefix=config.ckpt_prefix,\n",
    "    directory=os.path.join(ckpt_save_dir, 'ckpt_{}'.format(os.getenv('DEVICE_ID'))),\n",
    "    config=ckpt_config)\n",
    "loss_monitor = LossCallBack(rank_id=os.getenv('DEVICE_ID'))\n",
    "callbacks.append(loss_monitor)\n",
    "callbacks.append(ckpt_callback)\n",
    "\n",
    "print(\" | Start pre-training job.\")\n",
    "model.train(config.epochs, pre_train_dataset,\n",
    "            callbacks=callbacks, dataset_sink_mode=config.dataset_sink_mode,\n",
    "            sink_size=config.dataset_sink_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 微调\n",
    "\n",
    "微调时，根据不同的任务，采用不同的数据集对预训练的模型进行微调。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config()\n",
    "config.checkpoint_file_path = './output/checkpoint/ckpt_0/pt-21_80860.ckpt' #赋值给预训练生成的已有模型文件\n",
    "config.ckpt_prefix = \"ft\"\n",
    "config.epochs = 1 #for test\n",
    "\n",
    "print(\" | Starting training on single device.\")\n",
    "fine_tune_dataset = load_dataset(data_files=config.fine_tune_dataset,\n",
    "                                     batch_size=config.batch_size,\n",
    "                                     epoch_count=1,\n",
    "                                     sink_mode=config.dataset_sink_mode,\n",
    "                                     sink_step=config.dataset_sink_step)\n",
    "\n",
    "# 定义帶loss的網路\n",
    "net_with_loss = TransformerNetworkWithLoss(config, is_training=True)\n",
    "net_with_loss.init_parameters_data()\n",
    "\n",
    "# 读取已有模型文件的权重\n",
    "weights = load_checkpoint(config.checkpoint_file_path)\n",
    "for param in net_with_loss.trainable_params():\n",
    "    weights_name = param.name\n",
    "    if isinstance(weights[weights_name], Parameter):\n",
    "        param.set_data(weights[weights_name].data)\n",
    "    elif isinstance(weights[weights_name], Tensor):\n",
    "        param.set_data(Tensor(weights[weights_name].asnumpy(), config.dtype))\n",
    "    elif isinstance(weights[weights_name], np.ndarray):\n",
    "        param.set_data(Tensor(weights[weights_name], config.dtype))\n",
    "    else:\n",
    "        param.set_data(weights[weights_name])\n",
    "\n",
    "update_steps = config.epochs * fine_tune_dataset.get_dataset_size()\n",
    "\n",
    "# 定义递减的学习率\n",
    "lr = Tensor(polynomial_decay_scheduler(lr=config.lr,\n",
    "           min_lr=config.min_lr,\n",
    "           decay_steps=config.decay_steps,\n",
    "           total_update_num=update_steps,\n",
    "           warmup_steps=config.warmup_steps,\n",
    "           power=config.poly_lr_scheduler_power), dtype=mstype.float32)\n",
    "# 定义优化器\n",
    "optimizer = Adam(net_with_loss.trainable_params(), lr, beta1=0.9, beta2=0.98)\n",
    "\n",
    "# loss scale (mode = dynamic)\n",
    "scale_manager = DynamicLossScaleManager(init_loss_scale=config.init_loss_scale,\n",
    "                                        scale_factor=config.loss_scale_factor,\n",
    "                                        scale_window=config.scale_window)\n",
    "# 定义反向网络\n",
    "net_with_grads = TransformerTrainOneStepWithLossScaleCell(network=net_with_loss, optimizer=optimizer,\n",
    "                                                          scale_update_cell=scale_manager.get_update_cell())\n",
    "net_with_grads.set_train(True)\n",
    "\n",
    "# 初始化模型\n",
    "model = Model(net_with_grads)\n",
    "\n",
    "time_cb = TimeMonitor(data_size=fine_tune_dataset.get_dataset_size())\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=config.save_ckpt_steps,\n",
    "                               keep_checkpoint_max=config.keep_ckpt_max)\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(time_cb)\n",
    "ckpt_save_dir = config.save_checkpoint_path\n",
    "\n",
    "ckpt_callback = ModelCheckpoint(\n",
    "    prefix=config.ckpt_prefix,\n",
    "    directory=os.path.join(ckpt_save_dir, 'ckpt_{}'.format(os.getenv('DEVICE_ID'))),\n",
    "    config=ckpt_config)\n",
    "loss_monitor = LossCallBack(rank_id=os.getenv('DEVICE_ID'))\n",
    "callbacks.append(loss_monitor)\n",
    "callbacks.append(ckpt_callback)\n",
    "\n",
    "print(\" | Start fine-tuning job.\")\n",
    "model.train(config.epochs, fine_tune_dataset,\n",
    "            callbacks=callbacks, dataset_sink_mode=config.dataset_sink_mode,\n",
    "            sink_size=config.dataset_sink_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 推理\n",
    "测试过程中，通过微调后的模型预测结果，并采用波束大小為4的搜索算法获取可能性最高的预测结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import mindspore.nn as nn\n",
    "import mindspore.common.dtype as mstype\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.common.tensor import Tensor\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "\n",
    "from src.utils import Dictionary\n",
    "from src.dataset import load_dataset\n",
    "from src.transformer.transformer_for_infer import TransformerInferModel\n",
    "from src.transformer.transformer_for_train import TransformerTraining\n",
    "from src.utils.load_weights import load_infer_weights\n",
    "from src.utils.rouge_score import rouge\n",
    "\n",
    "class TransformerInferCell(nn.Cell):\n",
    "    def __init__(self, network):\n",
    "        super(TransformerInferCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "\n",
    "    def construct(self,\n",
    "                  source_ids,\n",
    "                  source_mask):\n",
    "        predicted_ids, predicted_probs = self.network(source_ids,\n",
    "                                                      source_mask)\n",
    "        return predicted_ids, predicted_probs\n",
    "\n",
    "def get_rouge_score(result, vocab):\n",
    "    \"\"\"Calculate ROUGE score.\"\"\"\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    for sample in result:\n",
    "        predictions.append(' '.join([vocab[t] for t in sample['prediction']]))\n",
    "        targets.append(' '.join([vocab[t] for t in sample['target']]))\n",
    "        print(f\" | source: {' '.join([vocab[t] for t in sample['source']])}\")\n",
    "        print(f\" | prediction: {predictions[-1]}\")\n",
    "        print(f\" | target: {targets[-1]}\")\n",
    "\n",
    "    return rouge(predictions, targets)\n",
    "\n",
    "config = config()\n",
    "config.epoch = 1 #for test\n",
    "config.checkpoint_file_path = \"./output/checkpoint/ckpt_0/ft_2-1_10319.ckpt\"\n",
    "\n",
    "vocab = Dictionary.load_from_persisted_dict(config.vocab)\n",
    "\n",
    "eval_dataset = load_dataset(data_files=config.test_dataset,\n",
    "                            batch_size=config.batch_size,\n",
    "                            epoch_count=1,\n",
    "                            sink_mode=config.dataset_sink_mode,\n",
    "                            shuffle=False)\n",
    "\n",
    "tfm_model = TransformerInferModel(config=config, use_one_hot_embeddings=False)\n",
    "tfm_model.init_parameters_data()\n",
    "\n",
    "params = tfm_model.trainable_params()\n",
    "weights = load_infer_weights(config)\n",
    "\n",
    "for param in params:\n",
    "    value = param.data\n",
    "    name = param.name\n",
    "    with open(\"weight_after_deal.txt\", \"a+\") as f:\n",
    "        weights_name = name\n",
    "        f.write(weights_name + \"\\n\")\n",
    "        if isinstance(value, Tensor):\n",
    "            if weights_name in weights:\n",
    "                assert weights_name in weights\n",
    "                param.set_data(Tensor(weights[weights_name], mstype.float32))\n",
    "            else:\n",
    "                raise ValueError(f\"{weights_name} is not found in checkpoint.\")\n",
    "        else:\n",
    "            raise TypeError(f\"Type of {weights_name} is not Tensor.\")\n",
    "\n",
    "print(\" | Load weights successfully.\")\n",
    "\n",
    "tfm_infer = TransformerInferCell(tfm_model)\n",
    "model = Model(tfm_infer)\n",
    "\n",
    "predictions = []\n",
    "probs = []\n",
    "source_sentences = []\n",
    "target_sentences = []\n",
    "for batch in eval_dataset.create_dict_iterator(output_numpy=True, num_epochs=1):\n",
    "    source_sentences.append(batch[\"source_eos_ids\"])\n",
    "    target_sentences.append(batch[\"target_eos_ids\"])\n",
    "\n",
    "    source_ids = Tensor(batch[\"source_eos_ids\"], mstype.int32)\n",
    "    source_mask = Tensor(batch[\"source_eos_mask\"], mstype.int32)\n",
    "\n",
    "    start_time = time.time()\n",
    "    predicted_ids, entire_probs = model.predict(source_ids, source_mask)\n",
    "    print(f\" | Batch size: {config.batch_size}, \"\n",
    "          f\"Time cost: {time.time() - start_time}.\")\n",
    "\n",
    "    predictions.append(predicted_ids.asnumpy())\n",
    "    probs.append(entire_probs.asnumpy())\n",
    "\n",
    "output = []\n",
    "for inputs, ref, batch_out, batch_probs in zip(source_sentences,\n",
    "                                               target_sentences,\n",
    "                                               predictions,\n",
    "                                               probs):\n",
    "    for i in range(config.batch_size):\n",
    "        if batch_out.ndim == 3:\n",
    "            batch_out = batch_out[:, 0]\n",
    "\n",
    "        example = {\n",
    "            \"source\": inputs[i].tolist(),\n",
    "            \"target\": ref[i].tolist(),\n",
    "            \"prediction\": batch_out[i].tolist(),\n",
    "            \"prediction_prob\": batch_probs[i].tolist()\n",
    "        }\n",
    "        output.append(example)\n",
    "\n",
    "with open(config.output, \"wb\") as f:\n",
    "    pickle.dump(output, f, 1)\n",
    "\n",
    "score = get_rouge_score(output, vocab)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## 3.7 评估指标\n",
    "\n",
    "### 3.7.1 PPL\n",
    "对于自然语言生成模型，最重要的问题就是生成的文本序列是否符合我们人类的使用习惯。\n",
    "在自然语言处理领域中，最常用的评估指标为PPL，perplexity（困惑度），它可以衡量语言模型的好坏，计算方法是根据每个词来估计一句话出现的概率，并用句子长度作标准化。\n",
    "\n",
    "$$\n",
    "PPL(S) = P(w_1w_2...w_N)^{-\\frac{1}{N}} \\\\\n",
    " = \\sqrt[N]{\\frac{1}{p(w_1 w_2...w_N)}} \\\\\n",
    " = \\sqrt[N]{\\Pi^N_{i=1} \\frac{1}{p(w_i | w_1w_2...w_{i-1})}}\n",
    "$$\n",
    "\n",
    "其中$S$代表输入的sentence，$N$为句子长度，$p(w_i)$是第$i$个词的概率。第一个词就是 $p(w_1|w_0)$，而$w_0$表示句子的起始，是个占位符。\n",
    "\n",
    "PPL越小，$p(w_i)$则越大，我们期望的sentence出现的概率就越高。Perplexity可以认为是average branch factor（平均分支系数），即预测下一个词时可以有多少个合理的选择，可选词数越少，表示生成的句子越接近目标，模型越准确。但perplexity只是大致估计下训练效果，它不是完全意义上的标准，具体问题还是要具体分析。\n",
    "\n",
    "### 3.7.2 ROUGE\n",
    "ROUGE指标是在机器翻译、自动摘要、问答生成等领域常见的评估指标，全称是 (Recall-Oriented Understudy for Gisting Evaluation)。ROUGE通过将模型生成的摘要或者回答与参考答案（一般是人工生成的）进行比较计算，得到对应的得分。ROUGE指标与BLEU指标非常类似，均可用来衡量生成结果和标准结果的匹配程度，不同的是ROUGE基于召回率，BLEU更看重准确率。\n",
    "\n",
    "在文本摘要微调任务上，使用 Gigaword 测试集计算ROUGE-1、ROUGE-2 和 ROUGE-L 的 F1 分数作为微调模型的评估结果。\n",
    "\n",
    "ROUGE-1和ROUGE-2属于ROUGE-N算法，就是将模型生成的结果和标准结果按N-gram拆分后，计算召回率。公式为：\n",
    "$$\n",
    "ROUGE-N = \\frac{\\sum_{S \\in ReferenceSummaries} \\sum_{gram_n \\in S} Count_{match}gram_n}{\\sum_{S \\in ReferenceSummaries} \\sum_{gram_n \\in S}Count(gram_n)}\n",
    "$$\n",
    "\n",
    "分子表示所有样本按N-gram拆分后与生成的结果按N-gram拆分后匹配上个数的和；分母表示所有样本的标准结果，按N-gram拆分后的和。\n",
    "\n",
    "Rouge-L的L意为: Longest Common Subsequence，最长公共子序列。Rouge-L的公式可以表示为：\n",
    "$$\n",
    "R_{lcs} = \\frac{LCS(X, Y)}{m}\\\\\n",
    "P_{lcs} = \\frac{LCS(X, Y)}{n}\\\\\n",
    "F_{lcs} = \\frac{(1+\\beta^2)R_{lcs}P_{lcs}}{R_{lcs}+\\beta^2 P_{lcs}}\n",
    "$$\n",
    "\n",
    "公式中的$F_{lcs}$就是ROUGE-L的得分。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.15.0",
   "language": "python",
   "name": "tensorflow-1.15.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
