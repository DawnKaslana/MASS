{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 基于MindSpore框架的MASS案例实现\n",
    "## 1 模型简介\n",
    "微软亚洲研究院于2019在ICML发表《MASS: Masked Sequence to Sequence Pre-training for Language Generation》，其借鑑了Bert的Masked Language Model预训练任务，提出了MAsked序列到序列预训练（MASS）模型，为语言生成任务联合预训练编码器和解码器。\n",
    "\n",
    "### 1.1 模型结构\n",
    "\n",
    "MASS 对句子随机屏蔽一个长度为 k 的连续片段，然后通过编码器 - 注意力 - 解码器模型预测生成该片段。<br>\n",
    "(1) 通过序列到序列框架预测被遮掩的token，MASS强制编码器理解未遮掩的token的含义，并鼓励解码器从编码器端提取有用信息。<br>\n",
    "(2) 通过在解码器端预测连续的标记，解码器可以比仅预测离散标记擁有更好的语言建模能力。<br>\n",
    "(3) 解码器的输入进一步遮掩了在编码器端未遮掩的內容（例如在预测片段 x3x4x5x6 时，仅将 x3x4x5 作为输入，其他token用 [M] 屏蔽）， 鼓励解码器从编码器端提取更多有用的信息，不只是利用已有的信息。<br>\n",
    "\n",
    "MASS的编码器-注意力-解码器结构，其中“_”表示被屏蔽掉的token。\n",
    "![](https://i.imgur.com/Jvhm0Dx.png)\n",
    "\n",
    "其模型基础结构可以使用任何Seq2Seq的结构，由于Transformer的优越性，故论文中使用6层Transformer Layer作为Encoder和Decoder的基础结构。\n",
    "\n",
    "### 1.2 模型特点\n",
    "MASS 有一个重要的超参数 k，表示屏蔽的连续片段长度，通过调整 k 的大小，MASS 能包含 BERT 中的掩碼语言模型训练方法以及 GPT 中标准的语言模型预训练方法，使 MASS 成为一个通用的预训练框架（当 K=1 或者 m 时，MASS 的概率形式分别和 BERT 中的掩碼语言模型以及 GPT 中的标准语言模型一致）。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 案例实现\n",
    "\n",
    "### 2.1 准备数据集\n",
    "案例实现中预训练模型所使用的数据即News Crawl的英语单语数据数据集，下载好的数据集为一纯文字文件，接下来需要对该数据进行预处理，预处理包括对数据进行分词、利用subword-nmt工具做bpe编码、对分词后的语料应用该bpe编码并构建词彙表等工作。\n",
    "\n",
    "而微调模型用于文本摘要任务所使用的数据集为Gigaword，该数据集已经有分割为训练、测试、验证集，有原文本(src)和目标摘要(tgt)两个文件，本案例只会使用训练及与测试集，数据集文件路径结构如下：\n",
    "\n",
    ".Dataset/<br>\n",
    "└── news_crawl<br>\n",
    "    └── news.2015.txt<br>\n",
    "└── ggw_data<br>\n",
    "    ├── test.src.txt<br>\n",
    "    ├── test.tgt.txt<br>\n",
    "    ├── train.src.txt<br>\n",
    "    └── train.tgt.txt<br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"對數據進行分詞\"\"\"\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "src_folder = \"/Users/dawnkaslana/Workspace/Dataset/news_crawl/\"\n",
    "out_folder = \"./tokenized_corpus/\"\n",
    "\n",
    "def create_tokenized_sentences(file_path, tokenized_file):\n",
    "    tokenized_sen = []\n",
    "    print(f\" | Processing {file_path}.\")\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for sen in file:\n",
    "            tokens = word_tokenize(sen)\n",
    "            tokens = [t for t in tokens if t != \" \"]\n",
    "            if len(tokens) > 175:\n",
    "                continue\n",
    "            tokenized_sen.append(\" \".join(tokens) + \"\\n\")\n",
    "\n",
    "    with open(tokenized_file, \"w\") as file:\n",
    "        file.writelines(tokenized_sen)\n",
    "    print(f\" | Wrote to {tokenized_file}.\")\n",
    "\n",
    "for file in os.listdir(src_folder):\n",
    "    if not file.endswith(\".txt\"):\n",
    "        continue\n",
    "    file_path = os.path.join(src_folder, file)\n",
    "    tokenized_file = os.path.join(out_folder, file.replace(\".txt\", \"_tokenized.txt\"))\n",
    "    create_tokenized_sentences(file_path, tokenized_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"利用subword-nmt工具生成bpe檔案\"\"\"\n",
    "src_folder_path = '/Users/dawnkaslana/Workspace/Dataset/news_crawl/' # source text folder path.\n",
    "os.system(\"cd %s && cat *.txt | subword-nmt learn-bpe -s 46000 -o all.bpe.codes\" % (src_folder_path))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"應用該bpe檔案並構建詞彙表.\"\"\"\n",
    "from src.utils import Dictionary\n",
    "import subprocess\n",
    "\n",
    "source_folder = os.path.abspath(\"./tokenized_corpus/\")\n",
    "output_folder = os.path.abspath(\"./tokenized_corpus/bpe/\")\n",
    "codes = os.path.abspath(\"./all.bpe.codes\")\n",
    "vocab_path = \"./vocab/vocab_en.dict.bin\"\n",
    "\n",
    "ENCODER = \"subword-nmt apply-bpe -c\"\n",
    "LEARN_DICT = \"subword-nmt get-vocab -i\"\n",
    "def bpe_encode(codes_path, src_path, output_path, dict_path):\n",
    "    # Encoding.\n",
    "    print(\" | Applying BPE encoding.\")\n",
    "    commands = ENCODER.split() + [codes_path] + [\"-i\"] + [src_path] + [\"-o\"] + [output_path]\n",
    "    subprocess.call(commands)\n",
    "    print(\" | Fetching vocabulary from single file.\")\n",
    "    # Learn vocab.\n",
    "    commands = LEARN_DICT.split() + [output_path] + [\"-o\"] + [dict_path]\n",
    "    subprocess.call(commands)\n",
    "\n",
    "available_dict = []\n",
    "for file in os.listdir(source_folder):\n",
    "    if file.endswith(\".txt\"):\n",
    "        output_path = os.path.join(output_folder, file.replace(\".txt\", \"_bpe.txt\"))\n",
    "        dict_path = os.path.join(output_folder, file.replace(\".txt\", \".dict\"))\n",
    "        available_dict.append(dict_path)\n",
    "        bpe_encode(codes, os.path.join(source_folder, file), output_path, dict_path)\n",
    "\n",
    "# 加载bpe_encode處理過的文本词汇表，行格式为word frequency。\n",
    "vocab = Dictionary.load_from_text(available_dict)\n",
    "vocab.persistence(vocab_path) #将词汇表对象保存为二进制文件。\n",
    "print(f\" | Vocabulary Size: {len(vocab)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 生成NewsCrawl數據集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Processing corpus /Users/dawnkaslana/Workspace/Dataset/news_crawl/news2007.txt.\n",
      " | Shortest len = 1.\n",
      " | Longest  len = 3269.\n",
      " | Total    sen = 2573547.\n",
      " | Write to /Users/dawnkaslana/Workspace/HWmodels/official/nlp/mass/train_data/news_crawl_dataset/news2007_len_32.tfrecord-001-of-001.\n",
      " | Generate Dataset for Pre-training is done.\n",
      " | Vocabulary size: 353.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create News Crawl Pre-Training Dataset.\"\"\"\n",
    "import os\n",
    "from src.dataset import MonoLingualDataLoader\n",
    "from src.language_model import LooseMaskedLanguageModel\n",
    "from src.utils import Dictionary\n",
    "\n",
    "input_folder_path = '/Users/dawnkaslana/Workspace/Dataset/news_crawl/'\n",
    "output_folder_path = './train_data/news_crawl_dataset/'\n",
    "vocab_path = './vocab/vocab_en.dict.bin'\n",
    "\n",
    "def create_pre_train(text_file, max_sen_len):\n",
    "    vocab = Dictionary.load_from_persisted_dict(vocab_path)\n",
    "\n",
    "    loader = MonoLingualDataLoader(\n",
    "        src_filepath=text_file,\n",
    "        lang=\"en\", dictionary=vocab,\n",
    "        language_model=LooseMaskedLanguageModel(mask_ratio=0.4, mask_all_prob=None),\n",
    "        max_sen_len=max_sen_len, min_sen_len=10\n",
    "    )\n",
    "\n",
    "    src_file_name = os.path.basename(text_file)\n",
    "\n",
    "    file_name = os.path.join(\n",
    "        output_folder_path,\n",
    "        src_file_name.replace('.txt', f'_len_{max_sen_len}.tfrecord')\n",
    "    )\n",
    "    loader.write_to_tfrecord(path=file_name)\n",
    "\n",
    "for file in os.listdir(input_folder_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        create_pre_train(os.path.join(input_folder_path, file), 32)\n",
    "\n",
    "print(f\" | Generate Dataset for Pre-training is done.\")\n",
    "print(f\" | Vocabulary size: {vocab.size}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 生成Gigaword數據集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Processing corpus /Users/dawnkaslana/Workspace/Dataset/ggw_data/org_data/train.src.txt.\n",
      " | Processing corpus /Users/dawnkaslana/Workspace/Dataset/ggw_data/org_data/train.tgt.txt.\n",
      " | Shortest len = 3.\n",
      " | Longest  len = 100.\n",
      " | Total    sen = 1981314.\n",
      " | Total token num=87383811, 87.10378401784284% replaced by <unk>.\n",
      " | Write to /Users/dawnkaslana/Workspace/HWmodels/official/nlp/mass/train_data/gigaword_dataset/gigaword_train_dataset.tfrecord-001-of-001.\n",
      " | Processing corpus /Users/dawnkaslana/Workspace/Dataset/ggw_data/org_data/test.src.txt.\n",
      " | Processing corpus /Users/dawnkaslana/Workspace/Dataset/ggw_data/org_data/test.tgt.txt.\n",
      " | Shortest len = 2.\n",
      " | Longest  len = 73.\n",
      " | Total    sen = 1081.\n",
      " | Total token num=46933, 85.92248524492362% replaced by <unk>.\n",
      " | Write to /Users/dawnkaslana/Workspace/HWmodels/official/nlp/mass/train_data/gigaword_dataset/gigaword_test_dataset.tfrecord-001-of-001.\n",
      " | Generate Dataset for fine-tuneing is done.\n",
      " | Vocabulary size: 353.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generate Gigaword dataset.\"\"\"\n",
    "import os\n",
    "from src.dataset import BiLingualDataLoader\n",
    "from src.language_model import NoiseChannelLanguageModel\n",
    "from src.utils import Dictionary\n",
    "\n",
    "input_folder_path = '/Users/dawnkaslana/Workspace/Dataset/ggw_data/'\n",
    "output_folder_path = './train_data/gigaword_dataset/'\n",
    "vocab_path = './vocab/vocab_en.dict.bin'\n",
    "\n",
    "vocab = Dictionary.load_from_persisted_dict(vocab_path)\n",
    "\n",
    "train = BiLingualDataLoader(\n",
    "    src_filepath=os.path.join(input_folder_path,\"train.src.txt\"),\n",
    "    tgt_filepath=os.path.join(input_folder_path,\"train.tgt.txt\"),\n",
    "    src_dict=vocab, tgt_dict=vocab,\n",
    "    src_lang=\"en\", tgt_lang=\"en\",\n",
    "    language_model=NoiseChannelLanguageModel(add_noise_prob=0.),\n",
    "    max_sen_len=32\n",
    ")\n",
    "\n",
    "train.write_to_tfrecord(\n",
    "    path=os.path.join(output_folder_path, \"gigaword_train_dataset.tfrecord\")\n",
    ")\n",
    "\n",
    "test = BiLingualDataLoader(\n",
    "    src_filepath=os.path.join(input_folder_path,\"test.src.txt\"),\n",
    "    tgt_filepath=os.path.join(input_folder_path,\"test.tgt.txt\"),\n",
    "    src_dict=vocab, tgt_dict=vocab,\n",
    "    src_lang=\"en\", tgt_lang=\"en\",\n",
    "    language_model=NoiseChannelLanguageModel(add_noise_prob=0),\n",
    "    max_sen_len=32\n",
    ")\n",
    "\n",
    "test.write_to_tfrecord(\n",
    "    path=os.path.join(output_folder_path, \"gigaword_test_dataset.tfrecord\")\n",
    ")\n",
    "\n",
    "print(f\" | Generate Dataset for fine-tuneing is done.\")\n",
    "print(f\" | Vocabulary size: {vocab.size}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 預訓練newscrawl數據\n",
    "python3 train.py --device_target GPU --output_path './output'\n",
    "要在機器上跑訓練和微調和推理才行\n",
    "用公用機寫這三點的檔案\n",
    "### 取得設定參數"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\"\"\"configuration \"\"\"\n",
    "import ml_collections\n",
    "def get_config():\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.dtype = 'float32' #only support float16 and float32\n",
    "    config.pre_train_dataset = os.path.abspath(\"./train_data/news_crawl_dataset/news2007_len_32.tfrecord-001-of-001\")\n",
    "    config.epochs = 20\n",
    "    config.batch_size = 64\n",
    "    config.checkpoint_path = \"./output/checkpoint/\"\n",
    "    config.checkpoint_path = \"\"\n",
    "    config.ckpt_prefix = \"ckpt\"\n",
    "    config.lr = 0.0001\n",
    "    config.seq_length = 32 #數據集的seq_length\n",
    "    config.vocab_size = 353 #數據集的vocab_size\n",
    "    config.hidden_size = 1024\n",
    "    config.beam_width = 4\n",
    "    config.optimizer = \"adam\"\n",
    "    config.metric = \"rouge\"\n",
    "    config.dataset_sink_mode = False\n",
    "    config.dataset_sink_step = 100\n",
    "    return config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from src.dataset import load_dataset\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "print(f\" | Starting training on {os.getenv('RANK_SIZE', None)} devices.\")\n",
    "\n",
    "pre_train_dataset = load_dataset(data_files=config.pre_train_dataset,\n",
    "                                     batch_size=config.batch_size,\n",
    "                                     epoch_count=1,\n",
    "                                     sink_mode=config.dataset_sink_mode,\n",
    "                                     sink_step=config.dataset_sink_step)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}